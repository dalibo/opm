#!/usr/bin/perl
# This program is open source, licensed under the simplified BSD license.
# For license terms, see the LICENSE file.

=head1 check_pgactivity

check_pgactivity - PostgreSQL plugins for Nagios

Version 1.0

=head1 SYNOPSIS

  check_pgactivity {-w|--warning THRESHOLD} {-c|--critical THRESHOLD} [-s|--service SERVICE ] [-h|--host HOST] [-U|--username ROLE] [-p|--port PORT] [-d|--dbname DATABASE] [-S|--dbservice SERVICE_NAME] [-P|--psql PATH] [--debug] [--status-file FILE] [--path PATH]
  check_pgactivity [--list]
  check_pgactivity [--help]

=head1 DESCRIPTION

check_pgactivity is dedicated to monitoring PostgreSQL cluster from Nagios. It
offers many different services and returns various usefull perfdata for
metrology.

=cut

use vars qw($VERSION $PROGRAM);

use strict;

use POSIX;
use File::Basename 'dirname';
use Getopt::Long qw(:config bundling no_ignore_case_always);
use List::Util qw(max);
use POSIX qw(locale_h sys_wait_h);
use IPC::Open3;
use Data::Dumper;
use Storable qw(store retrieve);
use Pod::Usage;
use File::Basename;

setlocale( LC_ALL, 'C' );

$| = 1;

$VERSION = '1.0';
$PROGRAM = 'check_pgactivity';

my $PG_VERSION_74 = 70400;
my $PG_VERSION_80 = 80000;
my $PG_VERSION_81 = 80100;
my $PG_VERSION_82 = 80200;
my $PG_VERSION_83 = 80300;
my $PG_VERSION_84 = 80400;
my $PG_VERSION_90 = 90000;
my $PG_VERSION_91 = 90100;
my $PG_VERSION_92 = 90200;
my $PG_VERSION_93 = 90300;

# available services and descriptions
my %services = (
    'backends' => {
        'sub'  => \&check_backends,
        'desc' => 'Number of connections, compared to max_connections.'
    },
    'database_size' => {
        'sub'  => \&check_database_size,
        'desc' => 'Variation of database sizes.',
        'min'  => $PG_VERSION_81
    },
    'wal_files' => {
        'sub'  => \&check_wal_files,
        'desc' => 'Total number of WAL files.',
        'min'  => $PG_VERSION_81
    },
    'ready_archives' => {
        'sub'  => \&check_ready_archives,
        'desc' => 'Check the number of wal files ready to archive.',
        'min'  => $PG_VERSION_81
    },
    'last_vacuum' => {
        'sub' => \&check_last_vacuum,
        'desc' =>
            'Check the oldest vacuum (from autovacuum or not) on the database.',
        'min' => $PG_VERSION_82
    },
    'last_analyze' => {
        'sub' => \&check_last_analyze,
        'desc' =>
            'Check the oldest analyze (from autovacuum or not) on the database.',
        'min' => $PG_VERSION_82
    },
    'locks' => {
        'sub'  => \&check_locks,
        'desc' => 'Check the number of locks on the hosts.'
    },
    'bgwriter' => {
        'sub'  => \&check_bgwriter,
        'desc' => 'Check the bgwriter activity.',
        'min'  => $PG_VERSION_83
    },
    'archive_folder' => {
        'sub'  => \&check_archive_folder,
        'desc' => 'Check archives in given folder.',
    },
    'minor_version' => {
        'sub'  => \&check_minor_version,
        'desc' => 'Check if the PostgreSQL minor version is the latest one.',
    },
    'hot_standby_delta' => {
        'sub'  => \&check_hot_standby_delta,
        'desc' => 'Check delta in bytes between a master and its Hot standbys.',
        'min'  => $PG_VERSION_90,
        'max'  => $PG_VERSION_90
    },
    'streaming_delta' => {
        'sub'  => \&check_streaming_delta,
        'desc' => 'Check delta in bytes between a master and its standbys in streaming replication.',
        'min'  => $PG_VERSION_91
    }
);

my %args = (
    'service'     => undef,
    'host'        => undef,
    'username'    => undef,
    'port'        => undef,
    'dbname'      => undef,
    'dbservice'   => undef,
    'warning'     => undef,
    'critical'    => undef,
    'exclude'     => '',
    'psql'        => '/usr/bin/psql',
    'path'        => undef,
    'status-file' => dirname(__FILE__) . '/check_pgactivity.data',
    'list'        => 0,
    'help'        => 0,
    'debug'       => 0
);

=over

=item B<-s>, B<--service> SERVICE

The nagios service to run. See section SERVICES for a description of available
services or option C<--list> for a short service and description list.

=item B<-h>, B<--host> HOST

Database server host or socket directory (default: "localhost").

=item B<-U>, B<--username> ROLE

Database user name (default: "postgres").

=item B<-p>, B<--port> PORT

Database server port (default: "5432").

=item B<-d>, B<--dbname> DATABASE

Database name to connect to (default: "postgres").

=item B<-S>, B<--dbservice> SERVICE_NAME

The service name to use from pg_service.conf to connect.

=item B<-w>, B<--warning> THRESHOLD

The warning threshold.

=item B<-c>, B<--critical> THRESHOLD

The critical threshold.

=item B<-P>, B<--psql> FILE

Path to the C<psql> executable (default: "/usr/bin/psql").

=item B<--status-file> PATH

PATH to the file where service status information will be kept between two
call. Default to check_pgactivity.data in the same directory of the script.

=item B<--list>

List available services.

=item B<-V>, B<--version>

Print version and exit.

=item B<--debug>

Print some debug messages.

=item B<-?>, B<--help>

Show this help page.

=back

THRESHOLD can either be a raw number, a size, an interval or a percentage.
Each available service supports one or more form (eg. a size and a percentage).

=cut

Getopt::Long::Configure('bundling');
GetOptions(
    \%args, qw{
        service|s=s
        host|h=s
        username|U=s
        port|p=s
        dbname|d=s
        dbservice|S=s
        warning|w=s
        critical|c=s
        psql|P=s
        path=s
        status-file=s
        list!
        version|V!
        help|?!
        debug!
        }
) or pod2usage( -exitval => 2 );

list_services() if $args{'list'};
version()       if $args{'version'};

pod2usage( -verbose => 2 ) if $args{'help'};


# Check that the given service exists.
pod2usage(
    -message => "FATAL: service $args{'service'} does not exist.\n"
        . "    Use --list to show the available services.",
    -exitval => 1
) unless exists $services{ $args{'service'} };

# One service must be given
pod2usage(
    -message => "FATAL: you must specify one service.\n"
        . "    See -s or --service command line option.",
    -exitval => 1
) unless defined $args{'service'};

# Critical and Warning must be given
pod2usage(
    -message => 'FATAL: you must specify critical and warning thresholds.',
    -exitval => 1
) unless ($args{'service'} eq 'minor_version')
    or (defined $args{'warning'} and $args{'critical'});

# We set defaults only if there is no dbservice file.
unless (defined $args{'dbservice'}) {
    $args{'host'} //= ($ENV{'PGHOST'} || 'localhost');
    $args{'username'} //= ($ENV{'PGUSER'} || 'postgres');
    $args{'port'} //= ($ENV{'PGPORT'} || '5432');
    $args{'dbname'} //= ($ENV{'PGDATABASE'} || 'template1');
}

# check_pgactivity might be call with several host informations
# Build the final @hosts array with connection informations for all given host.
my @hosts      = ();
my @dbhosts    = split( /,/, $args{'host'} );
my @dbnames    = split( /,/, $args{'dbname'} );
my @dbusers    = split( /,/, $args{'username'} );
my @dbports    = split( /,/, $args{'port'} );
my @dbservices = split( /,/, $args{'dbservice'} );
my $nbhosts    = max $#dbhosts, $#dbnames, $#dbusers, $#dbports;

# Take the first value for each connection properties as default.
# eg. "-h localhost -p 5432,5433" gives two hosts:
#    * localhost:5432
#    * localhost:5433
for ( my $i = 0; $i <= $nbhosts; $i++ ) {
    push(
        @hosts,
        {   'host'      => $dbhosts[$i]    || $dbhosts[0],
            'port'      => $dbports[$i]    || $dbports[0],
            'db'        => $dbnames[$i]    || $dbnames[0],
            'user'      => $dbusers[$i]    || $dbusers[0],
            'dbservice' => $dbservices[$i] || $dbservices[0],
            'pgversion' => undef
        }
    );

    $hosts[-1]{'name'} = sprintf('%shost=%s port=%d db=%s',
        defined $hosts[-1]{'service'}? "service=$hosts[-1]{'service'} ":'',
        $hosts[-1]{'host'}, $hosts[-1]{'port'}, $hosts[-1]{'db'}
    );
}

# build the psql command line for each host.
map {
    $_->{'psqlopts'} = [
        $args{'psql'}, '-qxXAtf', '-'
        ];
    push @{ $_->{'psqlopts'} }, ('-h', $_->{'host'}) if defined $_->{'host'};
    push @{ $_->{'psqlopts'} }, ('-p', $_->{'port'}) if defined $_->{'port'};
    push @{ $_->{'psqlopts'} }, ('-U', $_->{'user'}) if defined $_->{'user'};
} @hosts;

# Die on kill -2, -3 or -15
$SIG{'INT'} = $SIG{'QUIT'} = $SIG{'TERM'} = 'terminate';

# Set name of the program without path*
my $orig_name = $0;
$0 = $PROGRAM;

foreach my $host (@hosts) {

    unless ( is_service_compatible( $host, $args{'service'} ) ) {
        warn sprintf "Service %s is not compatible with host %s:%d (v%s).\n",
            $args{'service'}, $host->{'host'}, $host->{'port'}, $host->{'version'};
        exit 1;
    }

    dprint( sprintf "Execute service %s on %s:%d (v%s)\n",
        $args{'service'}, $host->{'host'}, $host->{'port'}, $host->{'version'} );
}

exit $services{ $args{'service'} }{'sub'}->( \@hosts, \%args );

# print the version and exit
sub version() {
    print "check_pgactivity version $VERSION\n";

    exit 0;
}

# List services that can be performed
sub list_services() {

    print "List of available services:\n\n";
    foreach my $service ( sort keys %services ) {
        printf "\t%-15s\t%s\n", $service, $services{$service}{'desc'};
    }
    exit 0;
}

# Record the given ref content for the given host in a file on disk.
# The file is defined by argument status-file on command line. By default:
#
#  dirname(__FILE__) . '/check_pgactivity.data'
#
# Format of data in this file is:
#   {
#     "${host}${port}" => {
#       "$name" => ref
#     }
#   }
# data can be retrieved later using the "load" sub.
sub save(%$$$) {
    my %host    = %{ shift() };
    my $name    = shift;
    my $ref     = shift;
    my $storage = shift;
    my $all     = {};
    my $hostkey = "$host{'host'}$host{'port'}";

    $all = retrieve($storage) if -f $storage;

    $all->{$hostkey}->{$name} = $ref;

    store( $all, $storage )
        or die "Can't store data in '$storage'!\n";
}

# Load the given ref content for the given host from the file on disk.
#
# See "save" sub comments for more info.
sub load(%$$) {
    my %host    = %{ shift() };
    my $name    = shift;
    my $storage = shift;
    my $hostkey = "$host{'host'}$host{'port'}";
    my $all;

    return undef unless -f $storage;

    $all = retrieve($storage);

    return $all->{$hostkey}->{$name};
}

# Returns formated time string with units.
# Takes a duration in seconds as parameter.
sub to_interval($) {
    my $val      = shift;
    my $interval = '';

    return $val if $val =~ /^-?inf/i;

    $val = int($val);

    if ( $val > 604800 ) {
        $interval = int( $val / 604800 ) . "w ";
        $val %= 604800;
    }

    if ( $val > 86400 ) {
        $interval .= int( $val / 86400 ) . "d ";
        $val %= 86400;
    }

    if ( $val > 3600 ) {
        $interval .= int( $val / 3600 ) . "h";
        $val %= 3600;
    }

    if ( $val > 60 ) {
        $interval .= int( $val / 60 ) . "m";
        $val %= 60;
    }

    $interval .= "${val}s" if $val > 0;

    return $interval;
}

=pod

If THRESHOLD is an interval, the following units are accepted (not case
sensitive): s (second), m (minute), h (hour), d (day). You can use more than
one unit per give value. If not set, the last unit is in seconds. Eg.: "1h 55m
6" = "1h55m6s".

=cut

# Takes an interval (with units) as parameter and returns a duration in second.
sub get_time($) {
    my $str_time = lc( shift() );
    my $ts       = 0;
    my @date;

    die(      "Malformed interval: «$str_time»!\n"
            . "Authorized unit are: dD, hH, mM, sS\n" )
        unless $str_time
            =~ /^\s*([0-9]{1,2}\s*[smhd]\s*)*[0-9]{1,2}\s*[smhd]?\s*$/;

    # no bad units should exists after this line!

    @date = split( /([smhd])/, $str_time );

LOOP_TS: while ( my $val = shift @date ) {

        $val = int($val) || die("Wrong value for an interval: «$val»!");
        my $unit = shift @date;

        if ( $unit eq 'm' ) {
            $ts += $val * 60;
            next LOOP_TS;
        }

        if ( $unit eq 'h' ) {
            $ts += $val * 3600;
            next LOOP_TS;
        }

        if ( $unit eq 'd' ) {
            $ts += $val * 86400;
            next LOOP_TS;
        }

        $ts += $val;
    }

    return $ts;
}

=pod

If THRESHOLD is a size, the following units are accepted (not case sensitive):
b (Byte), k (KB), m (MB), g (GB), t (TB), p (PB), e (EB) or Z (ZB). The factor
between units is 1024 Bytes. Eg. 1g = 1G = 1024*1024*1024.

=cut

# Takes a size with unit as parameter and returns it in bytes.
# If unit is '%', use the second parameter to compute the size in byte.
sub get_size($;$) {
    my $str_size = shift;
    my $size     = 0;
    my $unit     = '';

    $str_size =~ /^([0-9.]+)(.*)$/;

    $size = int($1);
    $unit = lc($2);

    return $size unless $unit ne '';

    if ( $unit eq '%' ) {
        my $ratio = shift;

        die("Can not compute a ratio without the factor!")
            unless defined $unit;

        return int( $size * $ratio / 100 );
    }

    return $size           if $unit eq 'b';
    return $size * 1024    if $unit eq 'k';
    return $size * 1024**2 if $unit eq 'm';
    return $size * 1024**3 if $unit eq 'g';
    return $size * 1024**4 if $unit eq 't';
    return $size * 1024**5 if $unit eq 'p';
    return $size * 1024**6 if $unit eq 'e';
    return $size * 1024**7 if $unit eq 'z';

    die("Unknown size unit: $unit");
}

# Execute a query on a host.
# Params:
#   * host
#   * query
#   * (optionnal) database
# The result is an array of array:
#   [
#     [column1, ...] # line1
#     ...
#   ]
sub query($$;$) {
    my $host  = shift;
    my $query = shift;
    my $pid;
    my $db = shift || $host->{'db'};
    my $rc;
    my $rnum = 0;
    my @res = ();
    my @err;

    dprint("$query\n");

    $ENV{PGSERVICE} = $host->{'dbservice'} if defined $host->{'dbservice'};

    $pid = open3( *PSQLIN, *PSQLOUT, *PSQLERR,
        ( @{ $host->{'psqlopts'} }, $db ) );

    print PSQLIN $query;

    close PSQLIN;

    waitpid( $pid, 0 );

    $rc = $? >> 8;

    @err = <PSQLERR>;

    dprint("rc: $rc\n");
    dprint( sprintf( "stderr: %d\n", scalar(@err) ) );

    unless ( $rc == 0 and scalar(@err) == 0 ) {
        exit unknown('CHECK_PGACTIVITY',
            [ "Query fail !\n" . join(" ", @err) ], []
        );
    }

    close PSQLERR;

    foreach my $field (<PSQLOUT>) {

        if ( $field =~ /^\s*$/ ) {
            $rnum++;
            next;
        }

        chomp($field);

        $field =~ /^[^|]+\|(.*)$/;
        push @{ $res[$rnum] }, $1;
    }

    dprint( Dumper( \@res ) );

    close PSQLOUT;

    return \@res;
}

# Select the query appropriate query amongs an hash of query according to the
# backend version and execute it. Same argument order than in "query" sub.
# Hash of query must be of this form:
#   {
#     pg_version_num => $query1,
#     ...
#   }
#
# Where pg_version_num is the minimum PostgreSQL version which can run the
# query. The given versions are in numric version. See "set_pgversion" about
# how to compute a PostgreSQL num version, or globals $PG_VERSION_*.
sub query_ver($%;$) {
    my $host    = shift;
    my %queries = %{ shift() };

    # shift returns undef if he db is not given. The value is then set in
    # "query" sub
    my $db = shift;

    foreach my $ver ( sort { $b cmp $a } keys %queries ) {
        return query( $host, $queries{$ver}, $db )
            if ( $ver <= $host->{'version_num'} );
    }

    return undef;
}

# Returns an array (not sorted) with all databases existing in given host but
# templates and "postgres" one.
sub get_all_dbname($) {
    my @dbs;

    push @dbs => $_->[0] foreach (
        @{  query(
                shift, q{
            SELECT datname
            FROM pg_database
            WHERE NOT datistemplate
                AND datname <> 'postgres';
        }
            )
        }
    );

    return \@dbs;
}

# check if given host can run the given service
sub is_service_compatible($$) {
    my $host    = shift;
    my $service = shift;
    my $ver = 100*int(get_pgversion_num($host)/100);

    $services{$service}{'min'} = $PG_VERSION_74
        if not defined $services{$service}{'min'};
    $services{$service}{'max'} = 9**9**9
        if not defined $services{$service}{'max'};

    return 1
        if $ver >= $services{$service}{'min'}
            and $ver <= $services{$service}{'max'};

    return 0;
}

# Query and set the version for the given host
sub set_pgversion($) {
    my $host = shift;

    unless ( $host->{'version'} ) {

        my $rs = query( $host, q{SELECT current_setting('server_version')} );

        if ( $? != 0 ) {
            &dprint("FATAL: psql error, $!\n");
            exit 1;
        }

        $host->{'version'} = $rs->[0][0];

        chomp( $host->{'version'} );
    }

    if ( $host->{'version'} =~ /^(\d+)\.(\d+)(.(\d+))?/ ) {
        $host->{'version_num'} = int($1) * 10000 + int($2) * 100 + int($4);

        return;
    }

    return 1;
}

# returns the human readable version of the given host
sub get_pgversion($) {
    my $host = shift;
    set_pgversion($host) unless $host->{'version'};

    return $host->{'version'} if $host->{'version'};

    return undef;
}

# returns the machine readable version (as integer) of the given host
sub get_pgversion_num($) {
    my $host = shift;

    set_pgversion($host) unless $host->{'version_num'};

    return $host->{'version_num'} if $host->{'version_num'};

    return undef;
}

sub dprint {
    return unless $args{'debug'};
    foreach (@_) {
        print "DEBUG: $_";
    }
}

sub unknown($@@) {
    return output( 3, $_[0], $_[1], $_[2], $_[3] );
}

sub critical($@@) {
    return output( 2, $_[0], $_[1], $_[2], $_[3] );
}

sub warning($@@) {
    return output( 1, $_[0], $_[1], $_[2], $_[3] );
}

sub ok($@@) {
    return output( 0, $_[0], $_[1], $_[2], $_[3] );
}

sub output ($$@@) {
    my $rc       = shift;
    my $check    = shift;
    my @msg      = @{ shift() };
    my @perfdata = @{ shift() };
    my $state    = 'OK';

    $state = 'WARNING'  if $rc == 1;
    $state = 'CRITICAL' if $rc == 2;
    $state = 'UNKNOWN'  if $rc == 3;

    printf( "%s %s: %s | %s",
        $check, $state,
        join( ', ', @msg ),
        join ' ' => @perfdata );

    return $rc;
}

=head1 SERVICES

Here is the list, descriptions and parameters of available services.

=over

=item B<backends> (all)

Check the total number of connexions on the cluster.

Perfdata contains the number of connexions per database.

Critical and Warning thresholds accept either a raw number or a percentage (eg.
80%). When a threshold is in percent, it is compared to the cluster parameter
C<max_connections>.

=cut

sub check_backends(@%) {

    my @rc;
    my @perfdata;
    my @msg;
    my $me           = 'POSTGRES_BACKENDS';
    my @hosts        = @{ $_[0] };
    my %args         = %{ $_[1] };
    my $num_backends = 0;
    my $sql          = q{SELECT datname, numbackends,
        current_setting('max_connections')
        FROM pg_stat_database};

    @rc = @{ query( $hosts[0], $sql ) };

    $args{'critical'} = int( $rc[0]->[2] * $args{'critical'} / 100 )
        if ( $args{'critical'} =~ /%$/ );

    $args{'warning'} = int( $rc[0]->[2] * $args{'warning'} / 100 )
        if ( $args{'warning'} =~ /%$/ );

    foreach my $db (@rc) {
        $num_backends += $db->[1];
        push @perfdata,
            sprintf( "%s=%d;%d;%d;%d;%d",
            $db->[0], $db->[1], $args{'warning'}, $args{'critical'}, 0,
            $db->[2] );
    }

    push @msg =>
        sprintf( "%d connections on %d", $num_backends, $rc[0]->[2] );

    return critical( $me, \@msg, \@perfdata )
        if $num_backends >= $args{'critical'};

    return warning( $me, \@msg, \@perfdata )
        if $num_backends >= $args{'warning'};

    return ok( $me, \@msg, \@perfdata );
}

=item B<database_size> (8.1+)

Check the variation of database sizes.

Perfdata contains the size of each database.

Critical and Warning thresholds accept either a raw number, a percentage or a
size (eg. 2.5G).

=cut

sub check_database_size(@%) {
    my @msg_crit;
    my @msg_warn;
    my @rs;
    my @perfdata;
    my %new_db_sizes;
    my %db_sizes
        = %{ load( $hosts[0], 'db_size', $args{'status-file'} ) || {} };
    my $me    = 'POSTGRES_DB_SIZE';
    my @hosts = @{ $_[0] };
    my %args  = %{ $_[1] };
    my $sql   = q{SELECT datname, pg_database_size(datname)
        FROM pg_database};

    @rs = @{ query( $hosts[0], $sql ) };

DB_LOOP: foreach my $db (@rs) {
        my $delta;
        my $w_limit = get_size( $args{'warning'},  $db->[1] );
        my $c_limit = get_size( $args{'critical'}, $db->[1] );
        $new_db_sizes{ $db->[0] } = $db->[1];
        $delta = $db->[1] - $db_sizes{ $db->[0] };

        push @perfdata,
            sprintf( "%s=%d;%d;%d", $db->[0], $db->[1], $w_limit, $c_limit );

        if ( abs($delta) >= $c_limit ) {
            push @msg_crit =>
                sprintf( "%s (%d, now: %d)", $db->[0], $delta, $db->[1] );
            next DB_LOOP;
        }

        if ( abs($delta) >= $w_limit ) {
            push @msg_warn =>
                sprintf( "%s (%d, now: %d)", $db->[0], $delta, $db->[1] );
            next DB_LOOP;
        }
    }

    save( $hosts[0], 'db_size', \%new_db_sizes, $args{'status-file'} );

    return critical( $me, [ @msg_crit, @msg_warn ], \@perfdata )
        if scalar @msg_crit > 0;

    return warning( $me, \@msg_warn, \@perfdata ) if scalar @msg_warn > 0;

    return ok( $me, [ scalar(@rs) . " database(s) checked" ], \@perfdata );
}

=item B<wal_files> (8.1+)

Check the number of wal files.

Perfdata returns the total number of wal files, current number of written wal
and the current number of recycled wal.

Critical and Warning thresholds accept either a raw number of file or a
percentage. In case of percentage, the limit is computed based on:

  100% = 1 + checkpoint_segments * (2 + checkpoint_completion_target)

or for PostgreSQL 8.1 and 8.2:

  100% = 1 + checkpoint_segments * 2
=cut

sub check_wal_files(@%) {
    my $wal_num;
    my $w_limit;
    my $c_limit;
    my @rs;
    my @perfdata;
    my @msg;
    my $me      = 'POSTGRES_WAL_FILES';
    my @hosts   = @{ $_[0] };
    my %args    = %{ $_[1] };
    my %queries = (
        $PG_VERSION_90 => q{
            SELECT count(*) AS count, sum(is_recycled::int) AS is_recycled,
              sum((NOT is_recycled)::int) AS written,
              CASE WHEN max_wal1 > max_wal2 THEN max_wal1 ELSE max_wal2 END
                AS max_wal
            FROM (
              SELECT file > first_value(file) OVER w AS is_recycled,
               1 + (
                current_setting('checkpoint_segments')::float4 *
                ( 2 + current_setting('checkpoint_completion_target')::float4 )
               ) AS max_wal1,
               1 + current_setting('wal_keep_segments')::float4 + 
                  current_setting('checkpoint_segments')::float4 AS max_wal2     
              FROM pg_ls_dir('pg_xlog') as file
              WHERE file ~ '^[0-9A-F]{24}$'
              WINDOW w AS (
                ORDER BY (pg_stat_file('pg_xlog/'||file)).modification
                DESC
              )
            ) AS t
            GROUP BY 4;},
        $PG_VERSION_84 => q{
            SELECT count(*) AS count, sum(is_recycled::int) AS is_recycled,
              sum((NOT is_recycled)::int) AS written,
              1 + (
                current_setting('checkpoint_segments')::float4 *
                ( 2 + current_setting('checkpoint_completion_target')::float4 )
              ) AS max_wal
            FROM (
              SELECT file > first_value(file) OVER w AS is_recycled
              FROM pg_ls_dir('pg_xlog') as file
              WHERE file ~ '^[0-9A-F]{24}$'
              WINDOW w AS (
                ORDER BY (pg_stat_file('pg_xlog/'||file)).modification
                DESC
              )
            ) AS t
            GROUP BY 4},
        $PG_VERSION_83 => q{
            SELECT count(*) AS num_file,
                sum(recycled::int) AS is_recycled,
                sum((NOT recycled)::int) AS written,
                1 + (
                    current_setting('checkpoint_segments')::float4 *
                    ( 2 + current_setting('checkpoint_completion_target')::float4 )
                )
            FROM (
              SELECT file, file > (
                SELECT s.f
                FROM pg_ls_dir('pg_xlog') AS s(f)
                ORDER BY (pg_stat_file('pg_xlog/'||s.f)).modification DESC
                LIMIT 1
              ) AS recycled
              FROM pg_ls_dir('pg_xlog') AS file
              WHERE file ~ '^[0-9A-F]{24}$'
            ) AS t},
        $PG_VERSION_81 => q{
            SELECT count(*) AS num_file,
                sum(recycled::int) AS is_recycled,
                sum((NOT recycled)::int) AS written,
                1 + (current_setting('checkpoint_segments')::integer * 2)
            FROM (
              SELECT file, file > (
                SELECT s.f
                FROM pg_ls_dir('pg_xlog') AS s(f)
                ORDER BY (pg_stat_file('pg_xlog/'||s.f)).modification DESC
                LIMIT 1
              ) AS recycled
              FROM pg_ls_dir('pg_xlog') AS file
              WHERE file ~ '^[0-9A-F]{24}$'
            ) AS t}
    );

    @rs = @{ query_ver( $hosts[0], \%queries )->[0] };

    $w_limit = get_size( $args{'warning'},  $rs[3] );
    $c_limit = get_size( $args{'critical'}, $rs[3] );

    push @perfdata => sprintf( "total_wal=%d;%d;%d;1;%d",
        $rs[0], $w_limit, $c_limit, $rs[3] );
    push @perfdata => sprintf( "recycled_wal=%d;%d;%d;0;%d",
        $rs[1], $w_limit, $c_limit, $rs[3] );
    push @perfdata => sprintf( "written_wal=%d;%d;%d;1;%d",
        $rs[2], $w_limit, $c_limit, $rs[3] );

    push @msg => "$rs[0] WAL files";

    return critical( $me, \@msg, \@perfdata ) if $rs[0] >= $c_limit;
    return warning( $me, \@msg, \@perfdata ) if $rs[0] >= $w_limit;
    return ok( $me, \@msg, \@perfdata );
}

=item B<ready_archives> (8.1+)

Check the number of wal files ready to archive.

Perfdata returns the number of wal files waiting to be archived.

Critical and Warning thresholds only accept a raw number of file
=cut

sub check_ready_archives(@%) {
    my $rs;
    my @perfdata;
    my @msg;
    my $me    = 'POSTGRES_READY_ARCHIVES';
    my @hosts = @{ $_[0] };
    my %args  = %{ $_[1] };
    my $query = q{
         SELECT count(*) AS count
         FROM pg_ls_dir('pg_xlog/archive_status') as file
         WHERE file ~ '^[0-9A-F]{24}.ready$' };

    $rs = query( $hosts[0], $query )->[0];

    push @perfdata => sprintf( "ready_archives=%d;%d;%d",
        $rs->[0], $args{'warning'}, $args{'critical'} );

    push @msg => "$rs->[0] WAL files ready to archive";

    return critical( $me, \@msg, \@perfdata ) if $rs->[0] >= $args{'critical'};
    return warning( $me, \@msg, \@perfdata ) if $rs->[0] >= $args{'warning'};
    return ok( $me, \@msg, \@perfdata );
}

# Agnostic check vacuum or analyze sub
sub check_last_maintenance($@%) {
    my $rs;
    my @perfdata;
    my @msg_crit;
    my @msg_warn;
    my $c_limit = get_time( $args{'critical'} );
    my $w_limit = get_time( $args{'warning'} );
    my @msg;
    my $type   = shift;
    my @hosts  = @{ $_[0] };
    my %args   = %{ $_[1] };
    my @all_db = @{ get_all_dbname( $hosts[0] ) };
    my $me     = 'POSTGRES_LAST_' . uc($type);
    my $query  = qq{
         SELECT min(
            coalesce(
                extract(epoch FROM current_timestamp -
                    CASE last_auto${type} > last_${type}
                        WHEN 't' THEN last_auto${type}
                        ELSE last_${type}
                    END
                )::float, '-infinity'::float
            ))
            FROM pg_stat_user_tables
    };

LOOP_DB: foreach my $db (@all_db) {
        my $rs = @{ query( $hosts[0], $query, $db )->[0] }[0];
        push @perfdata =>
            sprintf( '%s=%d;%d;%d', $db, $rs, $w_limit, $c_limit );

        if ( $rs =~ /^-inf/i or $rs >= $c_limit ) {
            push @msg_crit => "$db: " . to_interval($rs);
            next LOOP_DB;
        }

        if ( $rs >= $w_limit ) {
            push @msg_warn => "$db: " . to_interval($rs);
            next LOOP_DB;
        }
    }

    return critical( $me, [ @msg_crit, @msg_warn ], \@perfdata )
        if scalar @msg_crit > 0;

    return warning( $me, \@msg_warn, \@perfdata ) if scalar @msg_warn > 0;

    return ok( $me, [ scalar(@all_db) . " database(s) checked" ],
        \@perfdata );
}

=item B<last_analyze> (8.2+)

Check on each databases that the oldest analyze (from autovacuum or not) is not
older than the given threshold.

Perfdata returns oldest analyze per database in seconds.

Critical and Warning thresholds only accept an interval (eg. 1h30m25s).
=cut

sub check_last_analyze(@%) {
    return check_last_maintenance( 'analyze', @_ );
}

=item B<last_vacuum> (8.2+)

Check on each databases that the oldest vacuum (from autovacuum or not) is not
older than the given threshold.

Perfdata returns oldest vacuum per database in seconds.

Critical and Warning thresholds only accept an interval (eg. 1h30m25s).

=cut

sub check_last_vacuum(@$) {
    return check_last_maintenance( 'vacuum', @_ );
}

=item B<locks> (all)

Check the number of locks on the hosts.

Perfdata returns the number of lock for kind of lock.

Critical and Warning thresholds accept either a raw number of lock or a
percentage. In case of percentage, it is computed against the following limits
for 7.4 to 8.1:

  max_locks_per_transaction * max_connections

for 8.2+:

  max_locks_per_transaction * (max_connections + max_prepared_transactions)

=cut

sub check_locks(@%) {
    my @rs;
    my @perfdata;
    my @msg;
    my $total_locks = 0;
    my $me          = 'POSTGRES_LOCKS';
    my @hosts       = @{ $_[0] };
    my %args        = %{ $_[1] };
    my %queries     = (
        $PG_VERSION_74 => q{
            SELECT count(*), mode,
                current_setting('max_locks_per_transaction')::integer
                * current_setting('max_connections')::integer
            FROM pg_locks
            GROUP BY 2,3
        },
        $PG_VERSION_82 => q{
            SELECT count(*), mode,
                current_setting('max_locks_per_transaction')::integer * (
                    current_setting('max_prepared_transactions')::integer
                    + current_setting('max_connections')::integer)
            FROM pg_locks
            GROUP BY 2,3
        }
    );

    @rs = @{ query_ver( $hosts[0], \%queries ) };

    $args{'critical'} = $args{'critical'} * $rs[0][0] if $args{'critical'} =~ /%$/;
    $args{'warning'}  = $args{'warning'} * $rs[0][0]  if $args{'warning'}  =~ /%$/;

    map {
        $total_locks += $_->[0];
        push @perfdata => sprintf( "%s=%d;%d;%d",
            $_->[1], $_->[0], $args{'warning'}, $args{'critical'} );
    } @rs;

    push @msg => "$total_locks locks";

    return critical( $me, \@msg, \@perfdata )
        if $total_locks >= $args{'critical'};

    return warning( $me, \@msg, \@perfdata )
        if $total_locks >= $args{'warning'};

    return ok( $me, \@msg, \@perfdata );
}

=item B<bgwriter> (8.3+)

Check the percentage of pages written by backends since last check.

Perfdata contains pg_stat_bgwriter counters.

Critical and Warning thresholds only accept a percentage.

=cut

sub check_bgwriter(@%) {
    my @msg;
    my @msg_crit;
    my @msg_warn;
    my @rc;
    my @perfdata;
    my $delta_buffers_total;
    my $delta_buffers_backend;
    my $w_limit;
    my $c_limit;
    my %new_bgwriter;
    my %bgwriter
        = %{ load( $hosts[0], 'bgwriter', $args{'status-file'} ) || {} };
    my $me      = 'POSTGRES_BGWRITER';
    my @hosts   = @{ $_[0] };
    my %args    = %{ $_[1] };
    my %queries = (
        $PG_VERSION_83 => q{SELECT checkpoints_timed, checkpoints_req,
              buffers_checkpoint, buffers_clean, maxwritten_clean,
              buffers_backend, buffers_alloc, 0,
              current_setting('checkpoint_segments')
            FROM pg_stat_bgwriter;
        },
        $PG_VERSION_91 => q{SELECT checkpoints_timed, checkpoints_req,
              buffers_checkpoint, buffers_clean, maxwritten_clean,
              buffers_backend, buffers_alloc, buffers_backend_fsync,
              current_setting('checkpoint_segments')
            FROM pg_stat_bgwriter;
        }
    );

    @rc = @{ query_ver( $hosts[0], \%queries ) };

    $delta_buffers_total = $rc[0]->[2] - $bgwriter{'buffers_checkpoint'}
        + $rc[0]->[3] - $bgwriter{'buffers_clean'}
        + $rc[0]->[5] - $bgwriter{'buffers_backend'};

    $delta_buffers_backend = $rc[0]->[5] - $bgwriter{'buffers_backend'};

    $w_limit = get_size( $args{'warning'},  $delta_buffers_total );
    $c_limit = get_size( $args{'critical'}, $delta_buffers_total );

    push @perfdata,
        sprintf(
        'buffers_backend=%d;%d;%d checkpoint_timed=%d checkpoint_req=%d buffers_checkpoint=%d buffers_clean=%d maxwritten_clean=%d buffers_backend_fsync=%d buffers_alloc=%d',
        $rc[0]->[5], $w_limit,    $c_limit,    $rc[0]->[0], $rc[0]->[1],
        $rc[0]->[2], $rc[0]->[3], $rc[0]->[4], $rc[0]->[7], $rc[0]->[6]
        );

    $new_bgwriter{'checkpoint_timed'}      = $rc[0]->[0];
    $new_bgwriter{'checkpoint_req'}        = $rc[0]->[1];
    $new_bgwriter{'buffers_checkpoint'}    = $rc[0]->[2];
    $new_bgwriter{'buffers_clean'}         = $rc[0]->[3];
    $new_bgwriter{'maxwritten_clean'}      = $rc[0]->[4];
    $new_bgwriter{'buffers_backend'}       = $rc[0]->[5];
    $new_bgwriter{'buffers_backend_fsync'} = $rc[0]->[7];
    $new_bgwriter{'buffers_alloc'}         = $rc[0]->[6];

    save( $hosts[0], 'bgwriter', \%new_bgwriter, $args{'status-file'} );

    push @msg => sprintf( "%d%% of buffers written by buffers_backend",
        $delta_buffers_total == 0
        ? 0
        : 100 * $delta_buffers_backend / $delta_buffers_total );

    return critical( $me, \@msg, \@perfdata )
        if $delta_buffers_backend >= $c_limit;
    return warning( $me, \@msg, \@perfdata )
        if $delta_buffers_backend >= $w_limit;
    return ok( $me, \@msg, \@perfdata );
}


=item B<archive_folder>

Check if all archived WAL exist between the oldest and the latest WAL in the
archive folder and make sure they are 16MB.

This service requires the argument C<--path> on the command line to specify the
archive folder path to check.

Perfdata contains the number of WAL archived and the age of the latest one.

Critical and Warning define the max age of the latest archived WAL as an
interval (eg. 5m or 300s ).

=cut

sub check_archive_folder(@%) {
    my @msg;
    my @msg_crit;
    my @msg_warn;
    my @perfdata;
    my @filelist;
    my @filelist_sorted;
    my $w_limit;
    my $c_limit;
    my $timeline;
    my $wal;
    my $seg;
    my $latest_wal_age;
    my $dh;
    my $me      = 'POSTGRES_ARCHIVES';
    my @hosts   = @{ $_[0] };
    my %args    = %{ $_[1] };

    # "path" argument must be given
    pod2usage(
        -message => 'FATAL: you must specify the archive folder using "--path <dir>".',
        -exitval => 1
    ) unless defined $args{'path'};

    # invalid "path" argument
    pod2usage(
        -message => "FATAL: \"$args{'path'}\" is not a valid folder.",
        -exitval => 1
    ) unless -d $args{'path'};

    opendir( $dh, $args{'path'} )
        or die "Cannot opendir $args{'path'} : $!\n";

    my @filelist = map { [ $_ => (stat("$args{'path'}/$_"))[9,7] ] }
        grep( /[0-9A-F]{24}/, readdir($dh) );

    closedir($dh);

    $w_limit = get_time($args{'warning'});
    $c_limit = get_time($args{'critical'});

    # sort by mtime
    @filelist_sorted = sort { ($a->[1] <=> $b->[1]) || ($a->[0] cmp $b->[0]) } @filelist;

    $latest_wal_age = time() - $filelist_sorted[-1][1];

    push @perfdata,
        sprintf(
        'latest_archive_age=%ds;%d;%d num_archives=%d',
        $latest_wal_age, $w_limit, $c_limit,
        scalar @filelist_sorted
        );

    $timeline = hex(substr($filelist_sorted[-1][0], 0, 8));
    $wal = hex(substr($filelist_sorted[0][0], 8, 8));
    $seg = hex(substr($filelist_sorted[0][0], 16, 8));

    # check ALL archives are here.
    for (my $i = 0; $i <= $#filelist_sorted ; $i++) {
        my $curr = sprintf('%08X%08X%08X', $timeline, $wal + int(($seg + $i)/255), ($seg + $i)%255 );

        if ($curr ne $filelist_sorted[$i][0]) {
            push @msg => sprintf( 'Wrong sequence or file missing @ "%s"', $curr);
            last;
        }

        if ($filelist_sorted[$i][2] != 16777216) {
            push @msg => sprintf( '"%s" is not 16MB', $curr);
            last;
        }
    }

    return critical( $me, \@msg, \@perfdata ) if @msg;

    push @msg => sprintf( '%d WAL archived in "%s", latest archived since %s',
        scalar @filelist_sorted, $args{'path'}, to_interval($latest_wal_age));

    return critical( $me, \@msg, \@perfdata )
        if $latest_wal_age >= $c_limit;

    return warning( $me, \@msg, \@perfdata )
        if $latest_wal_age >= $w_limit;

    return ok( $me, \@msg, \@perfdata );
}


=item B<minor_version> (all)

Check if the cluster is running the latest minor available.

This service needs an internet access. Optionnaly, you can set the path to your
prefered program to access internet using the parameter "--path"
(eg. --path '/usr/bin/wget'). Supported programs are: GET, wget, curl, fetch,
lynx, links, links2.

Perfdata returns the numerical version of PostgreSQL.

Rise a critical alert if the minor version is not the latest. This service
ignores cirtical and warning arguments.

=cut

sub check_minor_version(@%) {
    my @perfdata;
    my @msg;
    my %latest_versions;
    my $rss;
    my $me    = 'POSTGRES_MINOR_VERSION';
    my @hosts = @{ $_[0] };
    my %args  = %{ $_[1] };
    my $major_version;
    my $timeout = 30;
    my $url = 'http://www.postgresql.org/versions.rss';

    # These methods comes from check_postgres,
    # by Greg Sabino Mullane <greg@endpoint.com>,
    # licenced under BSD
    our %get_methods = (
        'GET'    => "GET -t $timeout -H 'Pragma: no-cache' $url",
        'wget'   => "wget --quiet --timeout=$timeout --no-cache -O - $url",
        'curl'   => "curl --silent --max-time=$timeout -H 'Pragma: no-cache' $url",
        'fetch'  => "fetch -q -T $timeout -o - $url",
        'lynx'   => "lynx --connect-timeout=$timeout --dump $url",
        'links'  => 'links -dump $url',
        'links2' => 'links2 -dump $url'
    );

    # Force the fetching method
    if ($args{'path'}) {
        my $meth = basename $args{'path'};

        pod2usage(
            -message => "FATAL: \"$args{'path'}\" is not a valid program.",
            -exitval => 1
        ) unless -x $args{'path'};

        pod2usage(
            -message => "FATAL: \"$args{'path'}\" is not a supported program.",
            -exitval => 1
        ) unless $meth =~ 'GET|wget|curl|fetch|lynx|links|links2';
    }

    # fetch the latest versions
    foreach my $exe (values %get_methods) {
        $rss = qx{$exe 2>/dev/null};

        last if $rss =~ 'PostgreSQL latest versions';
    }

    return unknown($me, [ 'Could not fetch PostgreSQL latest versions' ], [])
        unless $rss;

    $latest_versions{"$1.$2"} = [$1 * 10000 + $2 * 100 + $3, "$1.$2.$3"]
        while $rss =~ /<title>(\d+)\.(\d+)\.(\d+)/g;

    $hosts[0]{'version'} =~ '^(\d+\.\d+).*$';
    $major_version = $1;

    unless ( defined $latest_versions{$major_version} ) {
        push @msg => sprintf('Unknown PostgreSQL version %s', $major_version );
        return unknown( $me, \@msg, [] );
    }

    push @perfdata => sprintf( "version=%d", $hosts[0]{'version_num'});

    if ( $hosts[0]{'version_num'} < $latest_versions{$major_version}[0] ) {
        push @msg => sprintf('Latest PostgreSQL is %s, running version is %s',
            $latest_versions{$major_version}[1],
            $hosts[0]{'version'}
        );
        return critical( $me, \@msg, \@perfdata )
    }

    push @msg => "PostgreSQL version $hosts[0]{'version'}";

    return ok( $me, \@msg, \@perfdata );
}


=item B<hot_standby_delta> (9.0)

Check the data delta between a cluster and its Hot standbys.

You must give two or more hosts' connection parameters.

Perfdata returns the data delta in bytes between the master and all given Hot
standbys.

Critical and Warning thresholds can takes one or two values separated by a
comma. If only one value given, it applies on both received and replayed data.
If two values given, the first one applies on received data, the second one on
replayed ones. These threshold only accept a size (eg. 2.5G).

This service rise a critical if it doesn't find exactly ONE cluster production
(ie. critical when 0 or 2 and more masters).

=cut

sub check_hot_standby_delta(@%) {
    my @perfdata;
    my @msg;
    my @msg_crit;
    my @msg_warn;
    my $master_location = '';
    my $num_clusters = 0;
    my $w_limit_received;
    my $c_limit_received;
    my $w_limit_replayed;
    my $c_limit_replayed;
    my $me    = 'POSTGRES_HOT_STANDBY_DELTA';
    my @hosts = @{ $_[0] };
    my %args  = %{ $_[1] };
    my $query = "
        SELECT (NOT pg_is_in_recovery())::int,
            CASE pg_is_in_recovery()
                WHEN 't' THEN pg_last_xlog_receive_location()
                ELSE pg_current_xlog_location()
            END,
            CASE pg_is_in_recovery()
                WHEN 't' THEN pg_last_xlog_replay_location()
                ELSE NULL::text
            END
    ";

    pod2usage(
        -message => 'FATAL: you must give two or more hosts with service "hot_standby_delta".',
        -exitval => 1
    ) if @hosts < 2;

    # fetch LSNs
    foreach my $host (@hosts) {
        $host->{'rs'} = \@{ query( $host, $query )->[0] };
        $num_clusters += $host->{'rs'}[0];
        $master_location = $host->{'rs'}[1] if $host->{'rs'}[0];
    }

    return critical($me, ['More than one cluster in production.'], [])
        if $num_clusters != 1;

    ($w_limit_received, $w_limit_replayed) = split /,/, $args{'warning'};
    ($c_limit_received, $c_limit_replayed) = split /,/, $args{'critical'};
    $w_limit_replayed //= $w_limit_received;
    $c_limit_replayed //= $c_limit_received;

    $w_limit_received = get_size( $w_limit_received );
    $c_limit_received = get_size( $c_limit_received );
    $w_limit_replayed = get_size( $w_limit_replayed );
    $c_limit_replayed = get_size( $c_limit_replayed );

    # we recycle this one to count the number of slave
    $num_clusters = 0;

    $master_location =~ m{^([0-9A-F]+)/([0-9A-F]+)$};
    $master_location = (hex('ff000000') * hex($1)) + hex($2);

    # compute deltas
    foreach my $host (@hosts) {
        next if $host->{'rs'}[0];
        my ($a, $b) = split(/\//, $host->{'rs'}[1]);
        $host->{'receive_delta'} = $master_location - (hex('ff000000') * hex($a)) - hex($b);

        ($a, $b) = split(/\//, $host->{'rs'}[2]);
        $host->{'replay_delta'} = $master_location - (hex('ff000000') * hex($a)) - hex($b);

        $host->{'name'} =~ s/ db=.*$//;

        push @perfdata => sprintf( "'receive delta %s'=%d 'replay delta %s'=%d ",
            $host->{'name'},
            $host->{'receive_delta'} > 0 ? $host->{'receive_delta'}:0,
            $host->{'name'},
            $host->{'replay_delta'} > 0 ? $host->{'replay_delta'}:0
        );

        if ($host->{'receive_delta'} > $c_limit_received) {
            push @msg_crit, "critical receive lag for $host->{'name'}";
            next;
        }

        if ($host->{'replay_delta'} > $c_limit_replayed) {
            push @msg_crit, "critical replay lag for $host->{'name'}";
            next;
        }

        if ($host->{'receive_delta'} > $w_limit_received) {
            push @msg_warn, "warning receive lag for $host->{'name'}";
            next;
        }

        if ($host->{'replay_delta'} > $w_limit_replayed) {
            push @msg_warn, "warning replay lag for $host->{'name'}";
            next;
        }

        $num_clusters++;
    }

    return critical( $me, [ @msg_crit, @msg_warn ], \@perfdata )
        if @msg_crit > 0;

    return warning( $me, \@msg_warn, \@perfdata ) if @msg_warn > 0;

    return ok($me, [ "$num_clusters Hot standby checked" ], \@perfdata);
}


=item B<streaming_delta> (9.1+)

Check the data delta between a cluster and its standbys in streaming replication.

Perfdata returns the data delta in bytes between the master and all standbys
found.

Critical and Warning thresholds can takes one or two values separated by a
comma. If only one value given, it applies on both flushed and replayed data.
If two values given, the first one applies on flushed data, the second one on
replayed ones. These threshold only accept a size (eg. 2.5G).

=cut

sub check_streaming_delta(@%) {
    my @perfdata;
    my @msg;
    my @msg_crit;
    my @msg_warn;
    my @rs;
    my $w_limit_flushed;
    my $c_limit_flushed;
    my $w_limit_replayed;
    my $c_limit_replayed;
    my $master_location = '';
    my $num_clusters = 0;
    my $me    = 'POSTGRES_STREAMING_DELTA';
    my @hosts = @{ $_[0] };
    my %args  = %{ $_[1] };
    my $query = "SELECT application_name, client_addr, pid,
            sent_location, write_location, flush_location, replay_location
        FROM pg_stat_replication";

    @rs = @{ query( $hosts[0], $query ) };

    return unknown($me, ['No slaves connected'], []) unless @rs;

    $rs[0][0] =~ m{^([0-9A-F]+)/([0-9A-F]+)$};
    $master_location = (hex('ff000000') * hex($1)) + hex($2);

    ($w_limit_flushed, $w_limit_replayed) = split /,/, $args{'warning'};
    ($c_limit_flushed, $c_limit_replayed) = split /,/, $args{'critical'};
    $w_limit_replayed //= $w_limit_flushed;
    $c_limit_replayed //= $c_limit_flushed;

    $w_limit_flushed = get_size( $w_limit_flushed );
    $c_limit_flushed = get_size( $c_limit_flushed );
    $w_limit_replayed = get_size( $w_limit_replayed );
    $c_limit_replayed = get_size( $c_limit_replayed );


    # compute deltas
    foreach my $host (@rs) {
        my $flush_delta;
        my $replay_delta;
        my $name;

        $host->[3] =~ m{^([0-9A-F]+)/([0-9A-F]+)$};
        $master_location = (hex('ff000000') * hex($1)) + hex($2);

        $host->[5] =~ m{^([0-9A-F]+)/([0-9A-F]+)$};
        $flush_delta = $master_location - (hex('ff000000') * hex($1)) - hex($2);

        $host->[6] =~ m{^([0-9A-F]+)/([0-9A-F]+)$};
        $replay_delta = $master_location - (hex('ff000000') * hex($1)) - hex($2);

        $name = sprintf('application_name=%s host=%s pid=%d',
            $host->[0], $host->[1], $host->[2]
        );

        push @perfdata => sprintf( "'flushed delta %s'=%d 'replay delta %s'=%d ",
            $name, $flush_delta, $name, $replay_delta
        );

        $num_clusters++;

        if ($flush_delta > $c_limit_flushed) {
            push @msg_crit, "critical flush lag for $name";
            next;
        }

        if ($replay_delta > $c_limit_replayed) {
            push @msg_crit, "critical replay lag for $name";
            next;
        }

        if ($flush_delta > $w_limit_flushed) {
            push @msg_warn, "warning flush lag for $name";
            next;
        }

        if ($replay_delta > $w_limit_replayed) {
            push @msg_warn, "warning replay lag for $name";
            next;
        }
    }

    return critical( $me, [ @msg_crit, @msg_warn ], \@perfdata )
        if @msg_crit > 0;

    return warning( $me, \@msg_warn, \@perfdata ) if @msg_warn > 0;

    return ok($me, [ "$num_clusters slaves checked" ], \@perfdata);
}

__END__

=back

=head1 EXAMPLES

=over

=item C<check_pgactivity -h localhost -p 5492 -s last_vacuum -w 30m -c 1h30m>

=item C<check_pgactivity -s database_size -w 10% -c 1G>

=back

=head1 LICENSING

This program is open source, licensed under the simplified BSD license. For
license terms, see the LICENSE provided with the sources.

=head1 AUTHORS

Author: Jehan-Guillaume de Rorthais
Copyright: (C) 2012 Jehan-Guillaume de Rorthais - All rights reserved.

Dalibo's team. http://www.dalibo.org
  
=cut
