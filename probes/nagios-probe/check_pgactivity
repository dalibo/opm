#!/usr/bin/perl
# This program is open source, licensed under the simplified BSD license.
# For license terms, see the LICENSE file.

=head1 check_pgactivity

check_pgactivity - PostgreSQL plugins for Nagios

Version 1.0

=head1 SYNOPSIS

  check_pgactivity {-w|--warning THRESHOLD} {-c|--critical THRESHOLD} [-s|--service SERVICE ] [-h|--host HOST] [-U|--username ROLE] [-p|--port PORT] [-d|--dbname DATABASE] [-S|--dbservice SERVICE_NAME] [-P|--psql PATH] [--debug] [--status-file FILE] [--path PATH]
  check_pgactivity [--list]
  check_pgactivity [--help]

=head1 DESCRIPTION

check_pgactivity is dedicated to monitoring PostgreSQL cluster from Nagios. It
offers many different services and returns various usefull perfdata for
metrology.

=cut

use vars qw($VERSION $PROGRAM);

use strict;
use warnings;

use POSIX;
use File::Basename 'dirname';
use Getopt::Long qw(:config bundling no_ignore_case_always);
use List::Util qw(max);
use POSIX qw(locale_h sys_wait_h);
use IPC::Open3;
use Data::Dumper;
use Storable qw(store retrieve);
use Pod::Usage;
use File::Basename;

setlocale( LC_ALL, 'C' );

$| = 1;

$VERSION = '1.0';
$PROGRAM = 'check_pgactivity';

my $PG_VERSION_MIN = 70400;
my $PG_VERSION_74  = 70400;
my $PG_VERSION_80  = 80000;
my $PG_VERSION_81  = 80100;
my $PG_VERSION_82  = 80200;
my $PG_VERSION_83  = 80300;
my $PG_VERSION_84  = 80400;
my $PG_VERSION_90  = 90000;
my $PG_VERSION_91  = 90100;
my $PG_VERSION_92  = 90200;
my $PG_VERSION_93  = 90300;
my $PG_VERSION_MAX = 9999999;

# Available services and descriptions.
#
# The referenced sub called to exec each service takes one parameters: a
# reference to the arguments hash (%args)
#
# Note that we can not use perl prototype for these subroutine as they are
# called indirectly (thus the args given by references).

my %services = (
    # 'service_name' => {
    #    'sub'     => sub reference to call to run this service
    #    'desc'    => 'a desctiption of the service'
    # }

    'backends' => {
        'sub'  => \&check_backends,
        'desc' => 'Number of connections, compared to max_connections.'
    },
    'database_size' => {
        'sub'  => \&check_database_size,
        'desc' => 'Variation of database sizes.',
    },
    'wal_files' => {
        'sub'  => \&check_wal_files,
        'desc' => 'Total number of WAL files.',
    },
    'ready_archives' => {
        'sub'  => \&check_ready_archives,
        'desc' => 'Check the number of wal files ready to archive.',
    },
    'last_vacuum' => {
        'sub' => \&check_last_vacuum,
        'desc' =>
            'Check the oldest vacuum (from autovacuum or not) on the database.',
    },
    'last_analyze' => {
        'sub' => \&check_last_analyze,
        'desc' =>
            'Check the oldest analyze (from autovacuum or not) on the database.',
    },
    'locks' => {
        'sub'  => \&check_locks,
        'desc' => 'Check the number of locks on the hosts.'
    },
    'longest_query' => {
        'sub'  => \&check_longest_query,
        'desc' => 'Check the longest running query.'
    },
    'bgwriter' => {
        'sub'  => \&check_bgwriter,
        'desc' => 'Check the bgwriter activity.',
    },
    'archive_folder' => {
        'sub'  => \&check_archive_folder,
        'desc' => 'Check archives in given folder.',
    },
    'minor_version' => {
        'sub'  => \&check_minor_version,
        'desc' => 'Check if the PostgreSQL minor version is the latest one.',
    },
    'hot_standby_delta' => {
        'sub'  => \&check_hot_standby_delta,
        'desc' => 'Check delta in bytes between a master and its Hot standbys.',
    },
    'streaming_delta' => {
        'sub'  => \&check_streaming_delta,
        'desc' => 'Check delta in bytes between a master and its standbys in streaming replication.',
    },
    'hit_ratio' => {
        'sub'  => \&check_hit_ratio,
        'desc' => 'Check hit ratio on databases.'
    },
    'backup_label_age' => {
        'sub'  => \&check_backup_label_age,
        'desc' => 'Check age of backup_label file.',
    },
    'connection' => {
        'sub'  => \&check_connection,
        'desc' => 'Perform a simple connection test.'
    },
    'custom_query' => {
        'sub'  => \&check_custom_query,
        'desc' => 'Perform the given user query.'
    },
    'configuration' => {
        'sub'  => \&check_configuration,
        'desc' => 'Check the most important settings.',
    },
    'bloat' => {
        'sub'  => \&check_bloat,
        'desc' => 'Check table and index bloat.'
    }
);


=over

=item B<-s>, B<--service> SERVICE

The nagios service to run. See section SERVICES for a description of available
services or option C<--list> for a short service and description list.

=item B<-h>, B<--host> HOST

Database server host or socket directory (default: "localhost").

=item B<-U>, B<--username> ROLE

Database user name (default: "postgres").

=item B<-p>, B<--port> PORT

Database server port (default: "5432").

=item B<-d>, B<--dbname> DATABASE

Database name to connect to (default: "postgres").

=item B<-S>, B<--dbservice> SERVICE_NAME

The service name to use from pg_service.conf to connect.

=item B<-w>, B<--warning> THRESHOLD

The warning threshold.

=item B<-c>, B<--critical> THRESHOLD

The critical threshold.

=item B<-P>, B<--psql> FILE

Path to the C<psql> executable (default: "/usr/bin/psql").

=item B<--status-file> PATH

PATH to the file where service status information will be kept between two
call. Default to check_pgactivity.data in the same directory of the script.

=item B<--list>

List available services.

=item B<-V>, B<--version>

Print version and exit.

=item B<--debug>

Print some debug messages.

=item B<-?>, B<--help>

Show this help page.

=back

=cut

my %args = (
    'service'               => undef,
    'host'                  => undef,
    'username'              => undef,
    'port'                  => undef,
    'dbname'                => undef,
    'dbservice'             => undef,
    'warning'               => undef,
    'critical'              => undef,
    'exclude'               => '',
    'psql'                  => '/usr/bin/psql',
    'path'                  => undef,
    'status-file'           => dirname(__FILE__) . '/check_pgactivity.data',
    'query'                 => undef,
    'type'                  => undef,
    'reverse'               => 0,
    'work_mem'              => undef,
    'maintenance_work_mem'  => undef,
    'shared_buffers'        => undef,
    'wal_buffers'           => undef,
    'checkpoint_segments'   => undef,
    'effective_cache_size'  => undef,
    'no_check_autovacuum'   => 0,
    'no_check_fsync'        => 0,
    'no_check_enable'       => 0,
    'no_check_track_counts' => 0,
    'list'                  => 0,
    'help'                  => 0,
    'debug'                 => 0
);

# Set name of the program without path*
my $orig_name = $0;
$0 = $PROGRAM;

# Die on kill -2, -3 or -15
$SIG{'INT'} = $SIG{'QUIT'} = $SIG{'TERM'} = 'terminate';

# print the version and exit
sub version() {
    print "check_pgactivity version $VERSION\n";

    exit 0;
}

# List services that can be performed
sub list_services() {

    print "List of available services:\n\n";

    foreach my $service ( sort keys %services ) {
        printf "\t%-17s\t%s\n", $service, $services{$service}{'desc'};
    }

    exit 0;
}

# Record the given ref content for the given host in a file on disk.
# The file is defined by argument "--status-file" on command line. By default:
#
#  dirname(__FILE__) . '/check_pgactivity.data'
#
# Format of data in this file is:
#   {
#     "${host}${port}" => {
#       "$name" => ref
#     }
#   }
# data can be retrieved later using the "load" sub.
#
# Parameters are :
#  * the host structure ref that holds the "host" and "port" parameters
#  * the name of the structure to save
#  * the ref of the structure to save
#  * the path to the file storage
sub save($$$$) {
    my $host    = shift;
    my $name    = shift;
    my $ref     = shift;
    my $storage = shift;
    my $all     = {};
    my $hostkey;

    if (defined $host->{'dbservice'}) {
        $hostkey = "$host->{'dbservice'}";
    }
    else {
        $hostkey = "$host->{'host'}$host->{'port'}";
    }

    $all = retrieve($storage) if -f $storage;

    $all->{$hostkey}{$name} = $ref;

    store( $all, $storage )
        or die "Can't store data in '$storage'!\n";
}

# Load the given ref content for the given host from the file on disk.
#
# See "save" sub comments for more info.
# Parameters are :
#  * the host structure ref that holds the "host" and "port" parameters
#  * the name of the structure to load
#  * the path to the file storage
sub load($$$) {
    my $host    = shift;
    my $name    = shift;
    my $storage = shift;
    my $hostkey;
    my $all;

    if (defined $host->{'dbservice'}) {
        $hostkey = "$host->{'dbservice'}";
    }
    else {
        $hostkey = "$host->{'host'}$host->{'port'}";
    }

    return undef unless -f $storage;

    $all = retrieve($storage);

    return $all->{$hostkey}{$name};
}

# Returns formated time string with units.
# Takes a duration in seconds as parameter.
sub to_interval($) {
    my $val      = shift;
    my $interval = '';

    return $val if $val =~ /^-?inf/i;

    $val = int($val);

    if ( $val > 604800 ) {
        $interval = int( $val / 604800 ) . "w ";
        $val %= 604800;
    }

    if ( $val > 86400 ) {
        $interval .= int( $val / 86400 ) . "d ";
        $val %= 86400;
    }

    if ( $val > 3600 ) {
        $interval .= int( $val / 3600 ) . "h";
        $val %= 3600;
    }

    if ( $val > 60 ) {
        $interval .= int( $val / 60 ) . "m";
        $val %= 60;
    }

    $interval .= "${val}s" if $val > 0;

    return $interval;
}

=head1 THRESHOLDS

THRESHOLD given as warning and critical values can either be a raw number, a
percentage, an interval or a size. Each available service supports one or more
form (eg. a size and a percentage).

=over

=item B<Percentage>

If threshold is a percentage, the value should finish with a '%' without space
with the actual value. Eg.: 95%.

=item B<Interval>

If THRESHOLD is an interval, the following units are accepted (not case
sensitive): s (second), m (minute), h (hour), d (day). You can use more than
one unit per give value. If not set, the last unit is in seconds. Eg.: "1h 55m
6" = "1h55m6s".

=cut

# Takes an interval (with units) as parameter and returns a duration in second.
sub get_time($) {
    my $str_time = lc( shift() );
    my $ts       = 0;
    my @date;

    die(      "Malformed interval: «$str_time»!\n"
            . "Authorized unit are: dD, hH, mM, sS\n" )
        unless $str_time
            =~ /^\s*([0-9]{1,2}\s*[smhd]\s*)*[0-9]{1,2}\s*[smhd]?\s*$/;

    # no bad units should exists after this line!

    @date = split( /([smhd])/, $str_time );

LOOP_TS: while ( my $val = shift @date ) {

        $val = int($val) || die("Wrong value for an interval: «$val»!");
        my $unit = shift(@date) || '';

        if ( $unit eq 'm' ) {
            $ts += $val * 60;
            next LOOP_TS;
        }

        if ( $unit eq 'h' ) {
            $ts += $val * 3600;
            next LOOP_TS;
        }

        if ( $unit eq 'd' ) {
            $ts += $val * 86400;
            next LOOP_TS;
        }

        $ts += $val;
    }

    return $ts;
}

=pod

=item B<Size>
If THRESHOLD is a size, the following units are accepted (not case sensitive):
b (Byte), k (KB), m (MB), g (GB), t (TB), p (PB), e (EB) or Z (ZB). The factor
between units is 1024 Bytes. Eg. 1g = 1G = 1024*1024*1024.

=back

=cut

# Takes a size with unit as parameter and returns it in bytes.
# If unit is '%', use the second parameter to compute the size in byte.
sub get_size($;$) {
    my $str_size = shift;
    my $size     = 0;
    my $unit     = '';

    $str_size =~ /^([0-9.]+)(.*)$/;

    $size = int($1);
    $unit = lc($2);

    return $size unless $unit ne '';

    if ( $unit eq '%' ) {
        my $ratio = shift;

        die("Can not compute a ratio without the factor!")
            unless defined $unit;

        return int( $size * $ratio / 100 );
    }

    return $size           if $unit eq 'b';
    return $size * 1024    if $unit =~ '^k[bo]?$';
    return $size * 1024**2 if $unit =~ '^m[bo]?$';
    return $size * 1024**3 if $unit =~ '^g[bo]?$';
    return $size * 1024**4 if $unit =~ '^t[bo]?$';
    return $size * 1024**5 if $unit =~ '^p[bo]?$';
    return $size * 1024**6 if $unit =~ '^e[bo]?$';
    return $size * 1024**7 if $unit =~ '^z[bo]?$';

    die("Unknown size unit: $unit");
}


=head1 CONNEXIONS

check_pgactivity allows two different of connexion specification: by service or
by specifying values for host, user, port and database. Moreover, some services
can run on multiple host or needs to connect to multiple ones.

You must specify one of the parameters bellow if the service need to connect
to your PostgreSQL instance. In other words, check_pgactivity will NOT look for
the libpq environment variables.

The rules with connexions parameters are:

=over

=item * Parameter C<--dbservice SERVICE_NAME>

Define a new host using the given service. Multiple hosts can be defined by
giving multiple services seperated by a comma. Eg.

  --dbservice service1,service2

=item * Parameters C<--host HOST>, C<--port PORT>, C<--user ROLE> or C<--dbname DATABASE>

One of these parameters is enough to defines a new host. If some other
parameters are missing, default values are used.

If multiple values are given, define as many host as maximum given values.

Values are associated by position. Eg.:

  --host h1,h2 --port 5432,5433

Means "host=h1 port=5432" and "host=h2 port=5433".

If the number of values is different between parameters, any host that miss a
parameter will use the first given value for this parameter. Eg.:

  --host h1,h2 --port 5433

Means: "host=h1 port=5433" and "host=h2 port=5433".

=item * Services are define first

As instance, giving:

  --dbservice s1 --host h1 --port 5433

Means "service=s1" and "host=h1 port=5433" in this order. If the service
supports only one host, the second is ignored

=item * Mutual exclusion between both methods

You can not overwrite services connexions variables with parameters C<--host HOST>, C<--port PORT>, C<--user ROLE> or C<--dbname DATABASE>

=back

=cut

sub parse_hosts(\%) {
    my %args = %{ shift() };
    my @hosts = ();

    if (defined $args{'dbservice'}) {
        push
            @hosts,
            {   'dbservice' => $_,
                'name'      => "service=$_",
                'pgversion' => undef
            }
        foreach split /,/, $args{'dbservice'};
    }


    # Add as many hosts than necessary depending on given parameters
    # host/port/db/user.
    # Any missing parameters will be set to its default value.
    if (defined $args{'host'}
        or defined $args{'username'}
        or defined $args{'port'}
        or defined $args{'dbname'}
    ) {
        $args{'host'} = $ENV{'PGHOST'} || 'localhost'
            unless defined $args{'host'};
        $args{'username'} = $ENV{'PGUSER'} || 'postgres'
            unless defined $args{'username'};
        $args{'port'} = $ENV{'PGPORT'} || '5432'
            unless defined $args{'port'};
        $args{'dbname'} = $ENV{'PGDATABASE'} || 'template1'
            unless defined $args{'dbname'};

        my @dbhosts = split( /,/, $args{'host'} );
        my @dbnames = split( /,/, $args{'dbname'} );
        my @dbusers = split( /,/, $args{'username'} );
        my @dbports = split( /,/, $args{'port'} );
        my $nbhosts = max $#dbhosts, $#dbnames, $#dbusers, $#dbports;

        # Take the first value for each connection properties as default.
        # eg. "-h localhost -p 5432,5433" gives two hosts:
        #    * localhost:5432
        #    * localhost:5433
        for ( my $i = 0; $i <= $nbhosts; $i++ ) {
            push(
                @hosts,
                {   'host'      => $dbhosts[$i] || $dbhosts[0],
                    'port'      => $dbports[$i] || $dbports[0],
                    'db'        => $dbnames[$i] || $dbnames[0],
                    'user'      => $dbusers[$i] || $dbusers[0],
                    'pgversion' => undef
                }
            );

            $hosts[-1]{'name'} = sprintf('host=%s port=%d db=%s',
                $hosts[-1]{'host'}, $hosts[-1]{'port'}, $hosts[-1]{'db'}
            );
        }
    }

    dprint ('Hosts: '. Dumper(\@hosts));

    return \@hosts;
}



# Execute a query on a host.
# Params:
#   * host
#   * query
#   * (optionnal) database
# The result is an array of array:
#   [
#     [column1, ...] # line1
#     ...
#   ]
sub query($$;$) {
    my $host  = shift;
    my $query = shift;
    my $db    = shift || $host->{'db'};
    my $rnum  = 0;
    my @res   = ();
    my @psqlopts = (
        $args{'psql'},
        '-qXAtf', '-',
        '-R', chr(30), # ASCII RS  (record separator)
        '-F', chr(3)   # ASCII ETX (end of text)
    );
    my $res;
    my @err;
    my $pid;
    my $rc;

    local $/ = undef;

    delete $ENV{PGSERVICE};
    delete $ENV{PGDATABASE};
    delete $ENV{PGHOST};
    delete $ENV{PGPORT};
    delete $ENV{PGUSER};

    if (defined $db) {
        $ENV{PGDATABASE} = $db;
    }
    elsif (defined $host->{'db'}) {
        $ENV{PGDATABASE} = $host->{'db'};
    }

    $ENV{PGSERVICE}  = $host->{'dbservice'} if defined $host->{'dbservice'};
    $ENV{PGHOST}     = $host->{'host'}      if defined $host->{'host'};
    $ENV{PGPORT}     = $host->{'port'}      if defined $host->{'port'};
    $ENV{PGUSER}     = $host->{'user'}      if defined $host->{'user'};

    dprint ("Query: $query\n");
    dprint ("Env. service: $ENV{PGSERVICE} \n") if defined $host->{'dbservice'};
    dprint ("Env. host   : $ENV{PGHOST}    \n") if defined $host->{'host'};
    dprint ("Env. port   : $ENV{PGPORT}    \n") if defined $host->{'port'};
    dprint ("Env. user   : $ENV{PGUSER}    \n") if defined $host->{'user'};
    dprint ("Env. db     : $ENV{PGDATABASE}\n") if defined $host->{'db'};

    $pid = open3( *PSQLIN, *PSQLOUT, *PSQLERR, @psqlopts);
    binmode PSQLOUT;

    print PSQLIN $query;

    close PSQLIN;

    waitpid( $pid, 0 );

    $rc = $? >> 8;

    @err = <PSQLERR>;

    dprint("Query rc: $rc\n");
    dprint( sprintf( "  # stderr: %u\n", scalar(@err) ) );

    unless ( $rc == 0 and scalar(@err) == 0 ) {
        exit unknown('CHECK_PGACTIVITY',
            [ "Query fail !\n" . join(" ", @err) ]
        );
    }

    close PSQLERR;

    $res = <PSQLOUT>;
    close PSQLOUT;

    if (defined $res) {
        chop $res;

        push @res, [ split(chr(3) => $_, -1) ]
            foreach split (chr(30) => $res, -1);
    }

    dprint( "Query result: ". Dumper( \@res ) );

    return \@res;
}

# Select the query appropriate query amongs an hash of query according to the
# backend version and execute it. Same argument order than in "query" sub.
# Hash of query must be of this form:
#   {
#     pg_version_num => $query1,
#     ...
#   }
#
# Where pg_version_num is the minimum PostgreSQL version which can run the
# query. The given versions are in numric version. See "set_pgversion" about
# how to compute a PostgreSQL num version, or globals $PG_VERSION_*.
sub query_ver($\%;$) {
    my $host    = shift;
    my %queries = %{ shift() };

    # shift returns undef if he db is not given. The value is then set in
    # "query" sub
    my $db = shift;

    set_pgversion($host);

    foreach my $ver ( sort { $b cmp $a } keys %queries ) {
        return query( $host, $queries{$ver}, $db )
            if ( $ver <= $host->{'version_num'} );
    }

    return undef;
}

# Returns an array (not sorted) with all databases existing in given host but
# templates and "postgres" one.
sub get_all_dbname($) {
    my @dbs;

    push @dbs => $_->[0] foreach (
        @{  query(
                shift, q{
            SELECT datname
            FROM pg_database
            WHERE NOT datistemplate
                AND datname <> 'postgres'
        }
            )
        }
    );

    return \@dbs;
}

# Query and set the version for the given host
sub set_pgversion($) {
    my $host = shift;

    unless ( $host->{'version'} ) {

        my $rs = query( $host, q{SELECT current_setting('server_version')} );

        if ( $? != 0 ) {
            dprint("FATAL: psql error, $!\n");
            exit 1;
        }

        $host->{'version'} = $rs->[0][0];

        chomp( $host->{'version'} );
    }

    if ( $host->{'version'} =~ /^(\d+)\.(\d+)(.(\d+))?/ ) {
        $host->{'version_num'} = int($1) * 10000 + int($2) * 100 + int($4);

        return;
    }

    return 1;
}

# Check host compatibility
sub is_compat($$$;$) {
    my $host    = shift;
    my $service = shift;
    my $min     = shift;
    my $max     = shift() || $PG_VERSION_MAX;
    my $ver;

    set_pgversion($host);
    $ver = 100*int($host->{'version_num'}/100);

    unless (
        $ver >= $min
        and $ver <= $max
    ) {
        warn sprintf "Service %s is not compatible with host '%s' (v%s).\n",
            $service, $host->{'name'}, $host->{'version'};
        return 0;
    }

    return 1;
}

sub dprint {
    return unless $args{'debug'};
    foreach (@_) {
        print "DEBUG: $_";
    }
}

sub unknown($;$$) {
    return output( 3, $_[0], $_[1], $_[2], $_[3] );
}

sub critical($;$$) {
    return output( 2, $_[0], $_[1], $_[2], $_[3] );
}

sub warning($;$$) {
    return output( 1, $_[0], $_[1], $_[2], $_[3] );
}

sub ok($;$$) {
    return output( 0, $_[0], $_[1], $_[2], $_[3] );
}

sub output ($$;$$) {
    my $rc  = shift;
    my $ret = shift;
    my $state;
    my @msg;
    my @perfdata;

    $ret .= " OK"       if $rc == 0;
    $ret .= " WARNING"  if $rc == 1;
    $ret .= " CRITICAL" if $rc == 2;
    $ret .= " UNKNOWN"  if $rc == 3;

    @msg      = @{ $_[0] } if defined $_[0];
    @perfdata = @{ $_[1] } if defined $_[1];

    $ret .= ": ".  join( ', ', @msg )     if @msg;
    $ret .= " | ". join( ' ', @perfdata ) if @perfdata;

    print $ret;

    return $rc;
}

=head1 SERVICES

Here is the list, descriptions and parameters of available services.

=over

=item B<backends> (all)

Check the total number of connexions on the cluster.

Perfdata contains the number of connexions per database.

Critical and Warning thresholds accept either a raw number or a percentage (eg.
80%). When a threshold is in percent, it is compared to the cluster parameter
C<max_connections>.

=cut

sub check_backends {

    my @rs;
    my @perfdata;
    my @msg;
    my @hosts;
    my %args         = %{ $_[0] };
    my $me           = 'POSTGRES_BACKENDS';
    my $num_backends = 0;
    my $sql          = q{SELECT datname, numbackends,
        current_setting('max_connections')
        FROM pg_stat_database};

    @hosts = @{ parse_hosts %args };

    pod2usage(
        -message => 'FATAL: you must give one host with service "backends".',
        -exitval => 127
    ) if @hosts != 1;

    @rs = @{ query( $hosts[0], $sql ) };

    $args{'critical'} = int( $rs[0][2] * $1 / 100 )
        if ( $args{'critical'} =~ /^([0-9.]+)%$/ );

    $args{'warning'} = int( $rs[0][2] * $1 / 100 )
        if ( $args{'warning'} =~ /^([0-9.]+)%$/ );

    foreach my $db (@rs) {
        $num_backends += $db->[1];
        push @perfdata,
            "$db->[0]=$db->[1];$args{'warning'};$args{'critical'};0;$db->[2]";
    }

    push @msg => "$num_backends connections on $rs[0][2]";

    return critical( $me, \@msg, \@perfdata )
        if $num_backends >= $args{'critical'};

    return warning( $me, \@msg, \@perfdata )
        if $num_backends >= $args{'warning'};

    return ok( $me, \@msg, \@perfdata );
}

=item B<database_size> (8.1+)

Check the variation of database sizes.

Perfdata contains the size of each database.

Critical and Warning thresholds accept either a raw number, a percentage or a
size (eg. 2.5G).

=cut

sub check_database_size {
    my @msg_crit;
    my @msg_warn;
    my @rs;
    my @perfdata;
    my %new_db_sizes;
    my @hosts;
    my %db_sizes;
    my %args = %{ $_[0] };
    my $me   = 'POSTGRES_DB_SIZE';
    my $sql  = q{SELECT datname, pg_database_size(datname)
        FROM pg_database};

    @hosts = @{ parse_hosts %args };

    pod2usage(
        -message => 'FATAL: you must give one host with service "database_size".',
        -exitval => 127
    ) if @hosts != 1;

    is_compat $hosts[0], 'database_size', $PG_VERSION_81 or exit 1;

    %db_sizes = %{ load( $hosts[0], 'db_size', $args{'status-file'} ) || {} };

    @rs = @{ query( $hosts[0], $sql ) };

DB_LOOP: foreach my $db (@rs) {
        my $delta;
        my $w_limit;
        my $c_limit;

        $new_db_sizes{ $db->[0] } = $db->[1];

        next DB_LOOP unless defined $db_sizes{ $db->[0] };

        $w_limit = get_size( $args{'warning'},  $db->[1] );
        $c_limit = get_size( $args{'critical'}, $db->[1] );
        $delta = $db->[1] - $db_sizes{ $db->[0] };

        push @perfdata => "$db->[0]=$db->[1]B;$w_limit;$c_limit";

        if ( abs($delta) >= $c_limit ) {
            push @msg_crit => "$db->[0] ($delta, now: $db->[1])";
            next DB_LOOP;
        }

        if ( abs($delta) >= $w_limit ) {
            push @msg_warn => "$db->[0] ($delta, now: $db->[1])";
            next DB_LOOP;
        }
    }

    save $hosts[0], 'db_size', \%new_db_sizes, $args{'status-file'};

    return critical( $me, [ @msg_crit, @msg_warn ], \@perfdata )
        if scalar @msg_crit > 0;

    return warning( $me, \@msg_warn, \@perfdata ) if scalar @msg_warn > 0;

    return ok( $me, [ scalar(@rs) . " database(s) checked" ], \@perfdata );
}

=item B<wal_files> (8.1+)

Check the number of wal files.

Perfdata returns the total number of wal files, current number of written wal
and the current number of recycled wal.

Critical and Warning thresholds accept either a raw number of file or a
percentage. In case of percentage, the limit is computed based on:

  100% = 1 + checkpoint_segments * (2 + checkpoint_completion_target)

or for PostgreSQL 8.1 and 8.2:

  100% = 1 + checkpoint_segments * 2
=cut

sub check_wal_files {
    my $wal_num;
    my $w_limit;
    my $c_limit;
    my @rs;
    my @perfdata;
    my @msg;
    my @hosts;
    my %args    = %{ $_[0] };
    my $me      = 'POSTGRES_WAL_FILES';
    my %queries = (
        $PG_VERSION_90 => q{
            SELECT count(*) AS count, sum(is_recycled::int) AS is_recycled,
              sum((NOT is_recycled)::int) AS written,
              CASE WHEN max_wal1 > max_wal2 THEN max_wal1 ELSE max_wal2 END
                AS max_wal
            FROM (
              SELECT file > first_value(file) OVER w AS is_recycled,
               1 + (
                current_setting('checkpoint_segments')::float4 *
                ( 2 + current_setting('checkpoint_completion_target')::float4 )
               ) AS max_wal1,
               1 + current_setting('wal_keep_segments')::float4 + 
                  current_setting('checkpoint_segments')::float4 AS max_wal2     
              FROM pg_ls_dir('pg_xlog') as file
              WHERE file ~ '^[0-9A-F]{24}$'
              WINDOW w AS (
                ORDER BY (pg_stat_file('pg_xlog/'||file)).modification
                DESC
              )
            ) AS t
            GROUP BY 4;},
        $PG_VERSION_84 => q{
            SELECT count(*) AS count, sum(is_recycled::int) AS is_recycled,
              sum((NOT is_recycled)::int) AS written,
              1 + (
                current_setting('checkpoint_segments')::float4 *
                ( 2 + current_setting('checkpoint_completion_target')::float4 )
              ) AS max_wal
            FROM (
              SELECT file > first_value(file) OVER w AS is_recycled
              FROM pg_ls_dir('pg_xlog') as file
              WHERE file ~ '^[0-9A-F]{24}$'
              WINDOW w AS (
                ORDER BY (pg_stat_file('pg_xlog/'||file)).modification
                DESC
              )
            ) AS t
            GROUP BY 4},
        $PG_VERSION_83 => q{
            SELECT count(*) AS num_file,
                sum(recycled::int) AS is_recycled,
                sum((NOT recycled)::int) AS written,
                1 + (
                    current_setting('checkpoint_segments')::float4 *
                    ( 2 + current_setting('checkpoint_completion_target')::float4 )
                )
            FROM (
              SELECT file, file > (
                SELECT s.f
                FROM pg_ls_dir('pg_xlog') AS s(f)
                ORDER BY (pg_stat_file('pg_xlog/'||s.f)).modification DESC
                LIMIT 1
              ) AS recycled
              FROM pg_ls_dir('pg_xlog') AS file
              WHERE file ~ '^[0-9A-F]{24}$'
            ) AS t},
        $PG_VERSION_81 => q{
            SELECT count(*) AS num_file,
                sum(recycled::int) AS is_recycled,
                sum((NOT recycled)::int) AS written,
                1 + (current_setting('checkpoint_segments')::integer * 2)
            FROM (
              SELECT file, file > (
                SELECT s.f
                FROM pg_ls_dir('pg_xlog') AS s(f)
                ORDER BY (pg_stat_file('pg_xlog/'||s.f)).modification DESC
                LIMIT 1
              ) AS recycled
              FROM pg_ls_dir('pg_xlog') AS file
              WHERE file ~ '^[0-9A-F]{24}$'
            ) AS t}
    );

    @hosts = @{ parse_hosts %args };

    pod2usage(
        -message => 'FATAL: you must give one host with service "wal_files".',
        -exitval => 127
    ) if @hosts != 1;

    is_compat $hosts[0], 'wal_files', $PG_VERSION_81 or exit 1;

    @rs = @{ query_ver( $hosts[0], %queries )->[0] };

    $w_limit = get_size( $args{'warning'},  $rs[3] );
    $c_limit = get_size( $args{'critical'}, $rs[3] );

    push @perfdata => "total_wal=$rs[0];$w_limit;$c_limit;1;$rs[3]";
    push @perfdata => "recycled_wal=$rs[1];$w_limit;$c_limit;0;$rs[3]";
    push @perfdata => "written_wal=$rs[2];$w_limit;$c_limit;1;$rs[3]";

    push @msg => "$rs[0] WAL files";

    return critical( $me, \@msg, \@perfdata ) if $rs[0] >= $c_limit;
    return warning( $me, \@msg, \@perfdata ) if $rs[0] >= $w_limit;
    return ok( $me, \@msg, \@perfdata );
}

=item B<ready_archives> (8.1+)

Check the number of wal files ready to archive.

Perfdata returns the number of wal files waiting to be archived.

Critical and Warning thresholds only accept a raw number of file
=cut

sub check_ready_archives {
    my @rs;
    my @perfdata;
    my @msg;
    my @hosts;
    my %args  = %{ $_[0] };
    my $me    = 'POSTGRES_READY_ARCHIVES';
    my $query = q{
         SELECT count(*) AS count
         FROM pg_ls_dir('pg_xlog/archive_status') as file
         WHERE file ~ '^[0-9A-F]{24}.ready$' };

    @hosts = @{ parse_hosts %args };

    pod2usage(
        -message => 'FATAL: you must give one host with service "ready_archives".',
        -exitval => 127
    ) if @hosts != 1;

    is_compat $hosts[0], 'ready_archives', $PG_VERSION_81 or exit 1;

    @rs = @{ query( $hosts[0], $query )->[0] };

    push @perfdata => "ready_archives=$rs[0];$args{'warning'};$args{'critical'}";

    push @msg => "$rs[0] WAL files ready to archive";

    return critical( $me, \@msg, \@perfdata ) if $rs[0] >= $args{'critical'};
    return warning( $me, \@msg, \@perfdata ) if $rs[0] >= $args{'warning'};
    return ok( $me, \@msg, \@perfdata );
}

# Agnostic check vacuum or analyze sub
sub check_last_maintenance {
    my $rs;
    my @perfdata;
    my @msg_crit;
    my @msg_warn;
    my @msg;
    my @hosts;
    my @all_db;
    my $type    = $_[0];
    my %args    = %{ $_[1] };
    my $c_limit = get_time( $args{'critical'} );
    my $w_limit = get_time( $args{'warning'} );
    my $me      = 'POSTGRES_LAST_' . uc($type);
    my $query   = qq{
         SELECT coalesce(min(
                extract(epoch FROM current_timestamp -
                    CASE last_auto${type} > last_${type}
                        WHEN 't' THEN last_auto${type}
                        ELSE last_${type}
                    END
                )::float
            ), 'NaN'::float)
            FROM pg_stat_user_tables
    };

    @hosts = @{ parse_hosts %args };

    pod2usage(
        -message => "FATAL: you must give one host with service \"last_$type\".",
        -exitval => 127
    ) if @hosts != 1;

    is_compat $hosts[0], "last_$type", $PG_VERSION_82 or exit 1;

    @all_db = @{ get_all_dbname( $hosts[0] ) };

LOOP_DB: foreach my $db (@all_db) {
        my $rs = @{ query( $hosts[0], $query, $db )->[0] }[0];
        push @perfdata => "$db=${rs}s;$w_limit;$c_limit";

        if ( $rs =~ /^-inf/i or $rs >= $c_limit ) {
            push @msg_crit => "$db: " . to_interval($rs);
            next LOOP_DB;
        }

        if ( $rs >= $w_limit ) {
            push @msg_warn => "$db: " . to_interval($rs);
            next LOOP_DB;
        }
    }

    return critical( $me, [ @msg_crit, @msg_warn ], \@perfdata )
        if scalar @msg_crit > 0;

    return warning( $me, \@msg_warn, \@perfdata ) if scalar @msg_warn > 0;

    return ok( $me, [ scalar(@all_db) . " database(s) checked" ], \@perfdata );
}

=item B<last_analyze> (8.2+)

Check on each databases that the oldest analyze (from autovacuum or not) is not
older than the given threshold.

Perfdata returns oldest analyze per database in seconds.

Critical and Warning thresholds only accept an interval (eg. 1h30m25s).
=cut

sub check_last_analyze {
    return check_last_maintenance( 'analyze', @_ );
}

=item B<last_vacuum> (8.2+)

Check on each databases that the oldest vacuum (from autovacuum or not) is not
older than the given threshold.

Perfdata returns oldest vacuum per database in seconds.

Critical and Warning thresholds only accept an interval (eg. 1h30m25s).

=cut

sub check_last_vacuum {
    return check_last_maintenance( 'vacuum', @_ );
}

=item B<locks> (all)

Check the number of locks on the hosts.

Perfdata returns the number of lock for kind of lock.

Critical and Warning thresholds accept either a raw number of lock or a
percentage. In case of percentage, it is computed against the following limits
for 7.4 to 8.1:

  max_locks_per_transaction * max_connections

for 8.2+:

  max_locks_per_transaction * (max_connections + max_prepared_transactions)

=cut

sub check_locks {
    my @rs;
    my @perfdata;
    my @msg;
    my @hosts;
    my %args        = %{ $_[0] };
    my $total_locks = 0;
    my $me          = 'POSTGRES_LOCKS';
    my %queries     = (
        $PG_VERSION_74 => q{
            SELECT count(l.*), ref.mode,
                current_setting('max_locks_per_transaction')::integer
                * current_setting('max_connections')::integer
            FROM (SELECT 'AccessShareLock'
                UNION SELECT 'RowShareLock'
                UNION SELECT 'RowExclusiveLock'
                UNION SELECT 'ShareUpdateExclusiveLock'
                UNION SELECT 'ShareLock'
                UNION SELECT 'ShareRowExclusiveLock'
                UNION SELECT 'ExclusiveLock'
                UNION SELECT 'AccessExclusiveLock'
            ) ref (mode)
            LEFT JOIN pg_locks l ON ref.mode = l.mode
            GROUP BY 2,3
        },
        $PG_VERSION_82 => q{
            SELECT count(l.*), ref.mode,
                current_setting('max_locks_per_transaction')::integer * (
                    current_setting('max_prepared_transactions')::integer
                    + current_setting('max_connections')::integer)
            FROM (SELECT * FROM ( VALUES
                ('AccessShareLock'),
                ('RowShareLock'),
                ('RowExclusiveLock'),
                ('ShareUpdateExclusiveLock'),
                ('ShareLock'),
                ('ShareRowExclusiveLock'),
                ('ExclusiveLock'),
                ('AccessExclusiveLock')
                ) lockmode (mode)
            ) ref
            LEFT JOIN pg_locks l ON ref.mode = l.mode
            GROUP BY 2,3
        }
    );

    @hosts = @{ parse_hosts %args };

    pod2usage(
        -message => 'FATAL: you must give one host with service "locks".',
        -exitval => 127
    ) if @hosts != 1;

    @rs = @{ query_ver $hosts[0], %queries };

    $args{'critical'} = int($1 * $rs[0][2]/100) if $args{'critical'} =~ /^([0-9.]+)%$/;
    $args{'warning'}  = int($1 * $rs[0][2]/100) if $args{'warning'}  =~ /^([0-9.]+)%$/;

    map {
        $total_locks += $_->[0];
        push @perfdata => ( "$_->[1]=$_->[0];$args{'warning'};$args{'critical'}" );
    } @rs;

    push @msg => "$total_locks locks";

    return critical( $me, \@msg, \@perfdata )
        if $total_locks >= $args{'critical'};

    return warning( $me, \@msg, \@perfdata )
        if $total_locks >= $args{'warning'};

    return ok( $me, \@msg, \@perfdata );
}

=item B<bgwriter> (8.3+)

Check the percentage of pages written by backends since last check.

Perfdata contains pg_stat_bgwriter counters.

Critical and Warning thresholds only accept a percentage.

=cut

sub check_bgwriter {
    my @msg;
    my @msg_crit;
    my @msg_warn;
    my @rs;
    my @perfdata;
    my $delta_buff_total;
    my $delta_buff_backend;
    my $delta_buff_bgwriter;
    my $delta_buff_checkpointer;
    my $delta_buff_backend_ratio;
    my $delta_buff_alloc;
    my $delta_checkpoint_timed;
    my $delta_checkpoint_req;
    my $delta_maxwritten_clean;
    my $delta_backend_fsync;
    my %new_bgw;
    my %bgw;
    my @hosts;
    my %args    = %{ $_[0] };
    my $me      = 'POSTGRES_BGWRITER';
    my %queries = (
        $PG_VERSION_83 => q{SELECT checkpoints_timed, checkpoints_req,
              buffers_checkpoint * current_setting('block_size')::numeric,
              buffers_clean * current_setting('block_size')::numeric,
              maxwritten_clean,
              buffers_backend * current_setting('block_size')::numeric,
              buffers_alloc * current_setting('block_size')::numeric,
              0,
              0
            FROM pg_stat_bgwriter;
        },
        $PG_VERSION_91 => q{SELECT checkpoints_timed, checkpoints_req,
              buffers_checkpoint * current_setting('block_size')::numeric,
              buffers_clean * current_setting('block_size')::numeric,
              maxwritten_clean,
              buffers_backend * current_setting('block_size')::numeric,
              buffers_alloc * current_setting('block_size')::numeric,
              buffers_backend_fsync,
              extract ('epoch' from stats_reset)
            FROM pg_stat_bgwriter;
        }
    );

    @hosts = @{ parse_hosts %args };

    pod2usage(
        -message => 'FATAL: you must give one host with service "bgwriter".',
        -exitval => 127
    ) if @hosts != 1;

    is_compat $hosts[0], 'bgwriter', $PG_VERSION_83 or exit 1;

    %bgw = %{ load( $hosts[0], 'bgwriter', $args{'status-file'} ) || {} };

    @rs = @{ query_ver( $hosts[0], %queries )->[0] };

    $new_bgw{'checkpoint_timed'} = $rs[0];
    $new_bgw{'checkpoint_req'}   = $rs[1];
    $new_bgw{'buff_checkpoint'}  = $rs[2];
    $new_bgw{'buff_clean'}       = $rs[3];
    $new_bgw{'maxwritten_clean'} = $rs[4];
    $new_bgw{'buff_backend'}     = $rs[5];
    $new_bgw{'buff_alloc'}       = $rs[6];
    $new_bgw{'backend_fsync'}    = $rs[7];
    $new_bgw{'stat_reset'}       = $rs[8];

    save $hosts[0], 'bgwriter', \%new_bgw, $args{'status-file'};

    return ok( $me, ['First call'] ) unless keys %bgw;
    return ok( $me, ['Stats reseted since last call'] )
        if $new_bgw{'stat_reset'}       > $bgw{'stat_reset'}
        or $new_bgw{'checkpoint_timed'} < $bgw{'checkpoint_timed'}
        or $new_bgw{'checkpoint_req'}   < $bgw{'checkpoint_req'}
        or $new_bgw{'buff_checkpoint'}  < $bgw{'buff_checkpoint'}
        or $new_bgw{'buff_clean'}       < $bgw{'buff_clean'}
        or $new_bgw{'maxwritten_clean'} < $bgw{'maxwritten_clean'}
        or $new_bgw{'buff_backend'}     < $bgw{'buff_backend'}
        or $new_bgw{'buff_alloc'}       < $bgw{'buff_alloc'}
        or $new_bgw{'backend_fsync'}    < $bgw{'backend_fsync'};

    $delta_buff_total = $rs[2] - $bgw{'buff_checkpoint'}
        + $rs[3] - $bgw{'buff_clean'}
        + $rs[5] - $bgw{'buff_backend'};

    $delta_buff_backend      = $rs[5] - $bgw{'buff_backend'};
    $delta_buff_bgwriter     = $rs[3] - $bgw{'buff_clean'};
    $delta_buff_checkpointer = $rs[2] - $bgw{'buff_checkpoint'};
    $delta_buff_alloc        = $rs[6] - $bgw{'buff_alloc'};
    $delta_checkpoint_timed  = $rs[0] - $bgw{'checkpoint_timed'};
    $delta_checkpoint_req    = $rs[1] - $bgw{'checkpoint_req'};
    $delta_maxwritten_clean  = $rs[4] - $bgw{'maxwritten_clean'};
    $delta_backend_fsync     = $rs[7] - $bgw{'backend_fsync'};

    push @perfdata, "buffers_backend=${delta_buff_backend}B".
                    " checkpoint_timed=${delta_checkpoint_timed}".
                    " checkpoint_req=${delta_checkpoint_req}".
                    " buffers_checkpoint=${delta_buff_checkpointer}B".
                    " buffers_clean=${delta_buff_bgwriter}B".
                    " maxwritten_clean=${delta_maxwritten_clean}".
                    " buffers_backend_fsync=${delta_backend_fsync}".
                    " buffers_alloc=${delta_buff_alloc}B";

    if ($delta_buff_total) {
        my $w_limit = get_size( $args{'warning'},  $delta_buff_total );
        my $c_limit = get_size( $args{'critical'}, $delta_buff_total );

        push @msg => sprintf(
            "%.2f%% from backends, %.2f%% from bgwriter, %.2f%% from checkpointer",
            100 * $delta_buff_backend      / $delta_buff_total,
            100 * $delta_buff_bgwriter     / $delta_buff_total,
            100 * $delta_buff_checkpointer / $delta_buff_total
        );

        $delta_buff_backend_ratio = 100
            * $delta_buff_backend / $delta_buff_total;
        return critical( $me, \@msg, \@perfdata )
            if $delta_buff_backend_ratio >= $c_limit;
        return warning( $me, \@msg, \@perfdata )
            if $delta_buff_backend_ratio >= $w_limit;
    }
    else {
        push @msg => "No writes";
    }

    return ok( $me, \@msg, \@perfdata );
}


=item B<archive_folder>

Check if all archived WAL exist between the oldest and the latest WAL in the
archive folder and make sure they are 16MB. The given folder must have archived
files from ONE cluster. The version of PostgreSQL that created the archives is
only checked on the last one for speed consideration.

This service requires the argument C<--path> on the command line to specify the
archive folder path to check.

Perfdata contains the number of WAL archived and the age of the latest one.

Critical and Warning define the max age of the latest archived WAL as an
interval (eg. 5m or 300s ).

=cut

sub check_archive_folder {
    my @msg;
    my @msg_crit;
    my @msg_warn;
    my @perfdata;
    my @filelist;
    my @filelist_sorted;
    my $w_limit;
    my $c_limit;
    my $timeline;
    my $wal;
    my $seg;
    my $latest_wal_age;
    my $dh;
    my $wal_version;
    my $me             = 'POSTGRES_ARCHIVES';
    my $seg_per_wal    = 255; # increased later for pg > 9.2
    my %args           = %{ $_[0] };
    my %wal_versions   = (
        '80' => 53340,
        '81' => 53341,
        '82' => 53342,
        '83' => 53346,
        '84' => 53347,
        '90' => 53348,
        '91' => 53350,
        '92' => 53361,
        '93' => 53365
    );

    # "path" argument must be given
    pod2usage(
        -message => 'FATAL: you must specify the archive folder using "--path <dir>".',
        -exitval => 127
    ) unless defined $args{'path'};

    # invalid "path" argument
    pod2usage(
        -message => "FATAL: \"$args{'path'}\" is not a valid folder.",
        -exitval => 127
    ) unless -d $args{'path'};

    opendir( $dh, $args{'path'} )
        or die "Cannot opendir $args{'path'} : $!\n";

    @filelist = map { [ $_ => (stat("$args{'path'}/$_"))[9,7] ] }
        grep( /^[0-9A-F]{24}$/, readdir($dh) );

    closedir($dh);

    $w_limit = get_time($args{'warning'});
    $c_limit = get_time($args{'critical'});

    # sort by mtime
    @filelist_sorted = sort { ($a->[1] <=> $b->[1]) || ($a->[0] cmp $b->[0]) } @filelist;

    $latest_wal_age = time() - $filelist_sorted[-1][1];

    # We need to read the XLOG_PAGE_MAGIC to be able to guess $seg_per_wal
    open FIRST_WAL, "<$args{'path'}/$filelist_sorted[-1][0]";
    read(FIRST_WAL, $wal_version, 2);
    close FIRST_WAL;
    $wal_version = unpack('S', $wal_version);

    dprint ("wal version: $wal_version\n");

    # FIXME: As there is no consensus about XLOG_PAGE_MAGIC algo accros postrges
    # versions this piece of code should checked for compatibility for each new
    # PostgreSQL version to confirm the new XLOG_PAGE_MAGIC is still greater
    # than the previous one (or at least the 9.2 one).
    $seg_per_wal++ if $wal_version >= $wal_versions{'93'};

    push @perfdata,
        "latest_archive_age=${latest_wal_age}s;$w_limit;$c_limit ".
        "num_archives=". scalar(@filelist_sorted);

    $timeline = hex(substr($filelist_sorted[-1][0], 0, 8));
    $wal = hex(substr($filelist_sorted[0][0], 8, 8));
    $seg = hex(substr($filelist_sorted[0][0], 16, 8));

    # check ALL archives are here.
    for (my $i = 0; $i <= $#filelist_sorted ; $i++) {
        dprint ("Checking WAL $filelist_sorted[$i][0]\n");
        my $curr = sprintf('%08X%08X%08X', $timeline, $wal + int(($seg + $i)/$seg_per_wal), ($seg + $i)%$seg_per_wal );

        if ($curr ne $filelist_sorted[$i][0]) {
            push @msg => "Wrong sequence or file missing @ '$curr'";
            last;
        }

        if ($filelist_sorted[$i][2] != 16777216) {
            push @msg => "'$curr' is not 16MB";
            last;
        }
    }

    return critical( $me, \@msg, \@perfdata ) if @msg;

    push @msg => scalar(@filelist_sorted)." WAL archived in '$args{'path'}', "
        ."latest archived since ". to_interval($latest_wal_age);

    return critical( $me, \@msg, \@perfdata )
        if $latest_wal_age >= $c_limit;

    return warning( $me, \@msg, \@perfdata )
        if $latest_wal_age >= $w_limit;

    return ok( $me, \@msg, \@perfdata );
}


=item B<minor_version> (all)

Check if the cluster is running the latest minor available.

This service needs an internet access. Optionnaly, you can set the path to your
prefered program to access internet using the parameter "--path"
(eg. --path '/usr/bin/wget'). Supported programs are: GET, wget, curl, fetch,
lynx, links, links2.

Perfdata returns the numerical version of PostgreSQL.

Rise a critical alert if the minor version is not the latest. This service
ignores critical and warning arguments.

=cut

sub check_minor_version {
    my @perfdata;
    my @msg;
    my %latest_versions;
    my $rss;
    my @hosts;
    my $major_version;
    my %args    = %{ $_[0] };
    my $me      = 'POSTGRES_MINOR_VERSION';
    my $timeout = 30;
    my $url     = 'http://www.postgresql.org/versions.rss';

    @hosts = @{ parse_hosts %args };

    pod2usage(
        -message => 'FATAL: you must give one host with service "bgwriter".',
        -exitval => 127
    ) if @hosts != 1;

    set_pgversion($hosts[0]);

    # These methods comes from check_postgres,
    # by Greg Sabino Mullane <greg@endpoint.com>,
    # licenced under BSD
    our %get_methods = (
        'GET'    => "GET -t $timeout -H 'Pragma: no-cache' $url",
        'wget'   => "wget --quiet --timeout=$timeout --no-cache -O - $url",
        'curl'   => "curl --silent --max-time=$timeout -H 'Pragma: no-cache' $url",
        'fetch'  => "fetch -q -T $timeout -o - $url",
        'lynx'   => "lynx --connect-timeout=$timeout --dump $url",
        'links'  => 'links -dump $url',
        'links2' => 'links2 -dump $url'
    );

    # Force the fetching method
    if ($args{'path'}) {
        my $meth = basename $args{'path'};

        pod2usage(
            -message => "FATAL: \"$args{'path'}\" is not a valid program.",
            -exitval => 127
        ) unless -x $args{'path'};

        pod2usage(
            -message => "FATAL: \"$args{'path'}\" is not a supported program.",
            -exitval => 127
        ) unless $meth =~ 'GET|wget|curl|fetch|lynx|links|links2';
    }

    # fetch the latest versions
    foreach my $exe (values %get_methods) {
        $rss = qx{$exe 2>/dev/null};

        last if $rss =~ 'PostgreSQL latest versions';
    }

    return unknown($me, [ 'Could not fetch PostgreSQL latest versions' ])
        unless $rss;

    $latest_versions{"$1.$2"} = [$1 * 10000 + $2 * 100 + $3, "$1.$2.$3"]
        while $rss =~ /<title>(\d+)\.(\d+)\.(\d+)/g;

    $hosts[0]{'version'} =~ '^(\d+\.\d+).*$';
    $major_version = $1;

    unless ( defined $latest_versions{$major_version} ) {
        push @msg => "Unknown PostgreSQL version $major_version";
        return unknown( $me, \@msg );
    }

    push @perfdata => "version=". $hosts[0]{'version_num'};

    if ( $hosts[0]{'version_num'} < $latest_versions{$major_version}[0] ) {
        push @msg => "Latest PostgreSQL is ".
            $latest_versions{$major_version}[1].
            "running version is ". $hosts[0]{'version'};
        return critical( $me, \@msg, \@perfdata )
    }

    push @msg => "PostgreSQL version ". $hosts[0]{'version'};

    return ok( $me, \@msg, \@perfdata );
}


=item B<hot_standby_delta> (9.0)

Check the data delta between a cluster and its Hot standbys.

You must give two or more hosts' connection parameters.

Perfdata returns the data delta in bytes between the master and all given Hot
standbys.

Critical and Warning thresholds can takes one or two values separated by a
comma. If only one value given, it applies on both received and replayed data.
If two values given, the first one applies on received data, the second one on
replayed ones. These threshold only accept a size (eg. 2.5G).

This service rise a critical if it doesn't find exactly ONE cluster production
(ie. critical when 0 or 2 and more masters).

=cut

sub check_hot_standby_delta {
    my @perfdata;
    my @msg;
    my @msg_crit;
    my @msg_warn;
    my $w_limit_received;
    my $c_limit_received;
    my $w_limit_replayed;
    my $c_limit_replayed;
    my @hosts;
    my %args  = %{ $_[0] };
    my $master_location = '';
    my $num_clusters = 0;
    my $me    = 'POSTGRES_HOT_STANDBY_DELTA';
    my $query = "
        SELECT (NOT pg_is_in_recovery())::int,
            CASE pg_is_in_recovery()
                WHEN 't' THEN pg_last_xlog_receive_location()
                ELSE pg_current_xlog_location()
            END,
            CASE pg_is_in_recovery()
                WHEN 't' THEN pg_last_xlog_replay_location()
                ELSE NULL::text
            END
    ";

    @hosts = @{ parse_hosts %args };

    pod2usage(
        -message => 'FATAL: you must give two or more hosts with service "hot_standby_delta".',
        -exitval => 127
    ) if @hosts < 2;

    is_compat $hosts[0], 'hot_standby_delta', $PG_VERSION_90, $PG_VERSION_90 or exit 1;

    # fetch LSNs
    foreach my $host (@hosts) {
        $host->{'rs'} = \@{ query( $host, $query )->[0] };
        $num_clusters += $host->{'rs'}[0];
        $master_location = $host->{'rs'}[1] if $host->{'rs'}[0];
    }

    return critical($me, ['More than one cluster in production.'])
        if $num_clusters != 1;

    ($w_limit_received, $w_limit_replayed) = split /,/, $args{'warning'};
    ($c_limit_received, $c_limit_replayed) = split /,/, $args{'critical'};

    if (!defined($w_limit_replayed)) {
        $w_limit_replayed = $w_limit_received;
    }
    if (!defined($c_limit_replayed)) {
        $c_limit_replayed = $c_limit_received;
    }

    $w_limit_received = get_size( $w_limit_received );
    $c_limit_received = get_size( $c_limit_received );
    $w_limit_replayed = get_size( $w_limit_replayed );
    $c_limit_replayed = get_size( $c_limit_replayed );

    # we recycle this one to count the number of slave
    $num_clusters = 0;

    $master_location =~ m{^([0-9A-F]+)/([0-9A-F]+)$};
    $master_location = (hex('ff000000') * hex($1)) + hex($2);

    # compute deltas
    foreach my $host (@hosts) {
        next if $host->{'rs'}[0];
        my ($a, $b) = split(/\//, $host->{'rs'}[1]);
        $host->{'receive_delta'} = $master_location - (hex('ff000000') * hex($a)) - hex($b);

        ($a, $b) = split(/\//, $host->{'rs'}[2]);
        $host->{'replay_delta'} = $master_location - (hex('ff000000') * hex($a)) - hex($b);

        $host->{'name'} =~ s/ db=.*$//;

        push @perfdata => "'receive delta $host->{'name'}'=".
            ($host->{'receive_delta'} > 0 ? $host->{'receive_delta'}:0).
            "B 'replay delta $host->{'name'}'=".
            ($host->{'replay_delta'} > 0 ? $host->{'replay_delta'}:0) .'B'
        ;

        if ($host->{'receive_delta'} > $c_limit_received) {
            push @msg_crit, "critical receive lag for $host->{'name'}";
            next;
        }

        if ($host->{'replay_delta'} > $c_limit_replayed) {
            push @msg_crit, "critical replay lag for $host->{'name'}";
            next;
        }

        if ($host->{'receive_delta'} > $w_limit_received) {
            push @msg_warn, "warning receive lag for $host->{'name'}";
            next;
        }

        if ($host->{'replay_delta'} > $w_limit_replayed) {
            push @msg_warn, "warning replay lag for $host->{'name'}";
            next;
        }

        $num_clusters++;
    }

    return critical( $me, [ @msg_crit, @msg_warn ], \@perfdata )
        if @msg_crit > 0;

    return warning( $me, \@msg_warn, \@perfdata ) if @msg_warn > 0;

    return ok($me, [ "$num_clusters Hot standby checked" ], \@perfdata);
}


=item B<streaming_delta> (9.1+)

Check the data delta between a cluster and its standbys in streaming replication.

Perfdata returns the data delta in bytes between the master and all standbys
found.

Critical and Warning thresholds can takes one or two values separated by a
comma. If only one value given, it applies on both flushed and replayed data.
If two values given, the first one applies on flushed data, the second one on
replayed ones. These threshold only accept a size (eg. 2.5G).

=cut

sub check_streaming_delta {
    my @perfdata;
    my @msg;
    my @msg_crit;
    my @msg_warn;
    my @rs;
    my $w_limit_flushed;
    my $c_limit_flushed;
    my $w_limit_replayed;
    my $c_limit_replayed;
    my @hosts;
    my %args            = %{ $_[0] };
    my $me              = 'POSTGRES_STREAMING_DELTA';
    my $master_location = '';
    my $num_clusters    = 0;
    my %queries         = (
        $PG_VERSION_92 => q{SELECT application_name, client_addr, pid,
            sent_location, write_location, flush_location, replay_location
            FROM pg_stat_replication},
        $PG_VERSION_91 => q{SELECT application_name, client_addr, procpid,
            sent_location, write_location, flush_location, replay_location
            FROM pg_stat_replication}
    );

    # FIXME this service should check for given slaves in opts!

    @hosts = @{ parse_hosts %args };

    pod2usage(
        -message => 'FATAL: you must give two or more hosts with service "streaming_delta".',
        -exitval => 127
    ) if @hosts != 1;

    is_compat $hosts[0], 'streaming_delta', $PG_VERSION_91 or exit 1;

    @rs = @{ query_ver( $hosts[0], %queries ) };

    return unknown($me, ['No slaves connected']) unless @rs;

    $rs[0][3] =~ m{^([0-9A-F]+)/([0-9A-F]+)$};
    $master_location = (hex('ff000000') * hex($1)) + hex($2);

    ($w_limit_flushed, $w_limit_replayed) = split /,/, $args{'warning'};
    ($c_limit_flushed, $c_limit_replayed) = split /,/, $args{'critical'};

    if (!defined($w_limit_replayed)) {
        $w_limit_replayed = $w_limit_flushed;
    }
    if (!defined($c_limit_replayed)) {
        $c_limit_replayed = $c_limit_flushed;
    }

    $w_limit_flushed = get_size( $w_limit_flushed );
    $c_limit_flushed = get_size( $c_limit_flushed );
    $w_limit_replayed = get_size( $w_limit_replayed );
    $c_limit_replayed = get_size( $c_limit_replayed );


    # compute deltas
    foreach my $host (@rs) {
        my $flush_delta;
        my $replay_delta;
        my $name;

        $host->[3] =~ m{^([0-9A-F]+)/([0-9A-F]+)$};
        $master_location = (hex('ff000000') * hex($1)) + hex($2);

        $host->[5] =~ m{^([0-9A-F]+)/([0-9A-F]+)$};
        $flush_delta = $master_location - (hex('ff000000') * hex($1)) - hex($2);

        $host->[6] =~ m{^([0-9A-F]+)/([0-9A-F]+)$};
        $replay_delta = $master_location - (hex('ff000000') * hex($1)) - hex($2);

        $name = "application_name=$host->[0] host=$host->[1] pid=$host->[2]";

        push @perfdata => "'flushed delta $name'=${flush_delta}B ".
            "'replay delta $name'=${replay_delta}B";

        $num_clusters++;

        if ($flush_delta > $c_limit_flushed) {
            push @msg_crit, "critical flush lag for $name";
            next;
        }

        if ($replay_delta > $c_limit_replayed) {
            push @msg_crit, "critical replay lag for $name";
            next;
        }

        if ($flush_delta > $w_limit_flushed) {
            push @msg_warn, "warning flush lag for $name";
            next;
        }

        if ($replay_delta > $w_limit_replayed) {
            push @msg_warn, "warning replay lag for $name";
            next;
        }
    }

    return critical( $me, [ @msg_crit, @msg_warn ], \@perfdata )
        if @msg_crit > 0;

    return warning( $me, \@msg_warn, \@perfdata ) if @msg_warn > 0;

    return ok($me, [ "$num_clusters slaves checked" ], \@perfdata);
}

=item B<hit_ratio> (all)

Check the cache hit ratio on the cluster.

Perfdata contains the hit ratio per database. Template databases and
databases that does not allow connections wont be checked, nor the
databases which has never been accessed.

Critical and Warning thresholds accept a percentage.
=cut
sub check_hit_ratio {
    my @rs;
    my @perfdata;
    my @msg_crit;
    my @msg_warn;
    my @hosts;
    my %args         = %{ $_[0] };
    my $me           = 'POSTGRES_HIT_RATIO';
    my $min_hit_ratio = 100000;
    my $sql          = q{SELECT d.datname,
        round((blks_hit::float/(blks_read+blks_hit+1)*100)::numeric, 2) as cachehitratio
        FROM pg_stat_database sd
        JOIN pg_database d ON d.oid = sd.datid
        WHERE d.datallowconn AND NOT d.datistemplate
        AND (blks_read+blks_hit) > 0
        ORDER BY datname, cachehitratio};

    @hosts = @{ parse_hosts %args };

    pod2usage(
        -message => 'FATAL: you must give one host with service "hit_ratio".',
        -exitval => 127
    ) if @hosts != 1;

    @rs = @{ query( $hosts[0], $sql ) };

DB_LOOP:    foreach my $db (@rs) {
        $min_hit_ratio = $db->[1] if ( $db->[1] < $min_hit_ratio );

        push @perfdata,
            "$db->[0]=$db->[1]%;$args{'warning'};$args{'critical'}";

        if ( $db->[1] < $args{'critical'} ) {
            push @msg_crit =>
                "$db->[0]: $db->[1]";
            next DB_LOOP;
        }
        if ( $db->[1] < $args{'warning'} ) {
            push @msg_warn =>
                "$db->[0]: $db->[1]";
            next DB_LOOP;
        }
    }

    return critical( $me, [ @msg_crit, @msg_warn ], \@perfdata )
        if $min_hit_ratio < $args{'critical'};

    return warning( $me, \@msg_warn, \@perfdata )
        if $min_hit_ratio < $args{'warning'};

    return ok( $me, [ scalar(@rs) . " database(s) checked" ], \@perfdata );
}

=item B<backup_label_age> (8.1+)

Check the age of the backup label file.

Perfdata returns the age of the backup_label file, -1 if not present.

Critical and Warning thresholds only accept an interval (eg. 1h30m25s).
=cut
sub check_backup_label_age {
    my $rs;
    my @perfdata;
    my @hosts;
    my %args    = %{ $_[0] };
    my $me      = 'POSTGRES_BACKUP_LABEL_AGE';
    my $c_limit = get_time( $args{'critical'} );
    my $w_limit = get_time( $args{'warning'} );
    my $sql     = q{SELECT max(s.r) AS value FROM (
            SELECT CAST(extract(epoch FROM current_timestamp - (pg_stat_file(file)).modification) AS integer) AS r
            FROM pg_ls_dir('.') AS ls(file)
            WHERE file='backup_label' UNION SELECT 0
        ) AS s};

    @hosts = @{ parse_hosts %args };

    pod2usage(
        -message => 'FATAL: you must give one host with service "backup_label_age".',
        -exitval => 127
    ) if @hosts != 1;

    is_compat $hosts[0], 'backup_label_age', $PG_VERSION_81 or exit 1;

    $rs = @{ query( $hosts[0], $sql )->[0] }[0];

    push @perfdata,
         "age=${rs}s;$w_limit;$c_limit";

    return critical( $me, [ "age: ".to_interval($rs) ], \@perfdata )
        if $rs > $c_limit;

    return warning( $me, [ "age: ".to_interval($rs) ], \@perfdata )
        if $rs > $w_limit;

    return ok( $me, [ "backup_label file ".( $rs == 0 ? "absent":"present (age: ".to_interval($rs).")") ], \@perfdata );
}


=item B<longest_query> (all)

Check the longest running query n the cluster. this service supports argument
C<--exclude REGEX> to exclude queries matching the given regexp from the check.

Perfdata contains the max/avg/min running time and the number of query per
databases.

Critical and Warning thresholds only accept an interval.
=cut

sub check_longest_query {
    my @rs;
    my @perfdata;
    my @msg;
    my @hosts;
    my $qr_exclude;
    my %args          = %{ $_[0] };
    my $c_limit       = get_time( $args{'critical'} );
    my $w_limit       = get_time( $args{'warning'} );
    my $me            = 'POSTGRES_LONGEST_QUERY';
    my $longest_query = 0;
    my $nb_query      = 0;
    my %stats         = ();
    my %queries       = (
       $PG_VERSION_74 => q{SELECT d.datname,
                COALESCE(elapsed, -1),
                COALESCE(query, '')
            FROM pg_database AS d
            LEFT JOIN (
                SELECT datname, current_query AS query,
                    extract('epoch' FROM
                        date_trunc('second', current_timestamp-query_start)
                    ) AS elapsed
                FROM pg_stat_activity
                WHERE current_query <> '<IDLE>'
            ) AS s ON (d.datname=s.datname)},

        $PG_VERSION_92 => q{SELECT d.datname,
                COALESCE(elapsed, 0),
                COALESCE(query, '')
            FROM pg_database AS d
            LEFT JOIN (
                SELECT datname, query,
                    extract('epoch' FROM
                        date_trunc('second', current_timestamp-state_change)
                    ) AS elapsed
                FROM pg_stat_activity
                WHERE state = 'active'
            ) AS s ON (d.datname=s.datname)}
    );

    @hosts = @{ parse_hosts %args };

    pod2usage(
        -message => 'FATAL: you must give one host with service "longest_query".',
        -exitval => 127
    ) if @hosts != 1;

    $qr_exclude = qr/$args{'exclude'}/ if $args{'exclude'};

    @rs = @{ query_ver( $hosts[0], %queries ) };

    REC_LOOP: foreach my $r (@rs) {
        next REC_LOOP if $qr_exclude and $r->[2] =~ $qr_exclude;

        $stats{$r->[0]} = {
            'num' => 0,
            'max' => -1,
            'avg' => 0,
        } unless exists $stats{$r->[0]};

        next REC_LOOP unless $r->[2] ne '';

        $longest_query = $r->[1] if $r->[1] > $longest_query;
        $nb_query++;

        $stats{$r->[0]}{'num'}++;
        $stats{$r->[0]}{'max'} = $r->[1] if $stats{$r->[0]}{'max'} < $r->[1];
        $stats{$r->[0]}{'avg'} = (
            $stats{$r->[0]}{'avg'} * ($stats{$r->[0]}{'num'} -1) + $r->[1])
            / $stats{$r->[0]}{'num'};
    }

    DB_LOOP: foreach my $db (keys %stats) {

        unless($stats{$db}{'max'} > -1) {
            $stats{$db}{'max'} = 'NaN';
            $stats{$db}{'avg'} = 'NaN';
        }

        push @perfdata, (
            "'$db max'=$stats{$db}{'max'}s;$w_limit;$c_limit",
            "'$db avg'=$stats{$db}{'avg'}s;$w_limit;$c_limit",
            "'$db #queries'=$stats{$db}{'num'}"
        );

        if ( $stats{$db}{'max'} > $c_limit ) {
            push @msg => "$db: ". to_interval($stats{$db}{'max'});
            next DB_LOOP;
        }

        if ( $stats{$db}{'max'} > $w_limit ) {
            push @msg => "$db: ". to_interval($stats{$db}{'max'});
        }
    }

    return critical( $me, \@msg, \@perfdata )
        if $longest_query > $c_limit;

    return warning( $me, \@msg, \@perfdata )
        if $longest_query > $w_limit;

    return ok( $me, [ "$nb_query running querie(s)" ], \@perfdata );
}

=item B<connection> (all)

Perform a simple connection test.

No perfdata is returned.

This service ignore critical and warning arguments.
=cut
sub check_connection {
    my @rs;
    my @hosts;
    my %args = %{ $_[0] };
    my $me   = 'POSTGRES_CONNECTION';
    my $sql  = q{SELECT now(), version()};

    @hosts = @{ parse_hosts %args };

    pod2usage(
        -message => 'FATAL: you must give one host with service "connection".',
        -exitval => 127
    ) if @hosts != 1;

    @rs = @{ query( $hosts[0], $sql ) };

    return ok( $me, [ "Connection successful at $rs[0][0], on $rs[0][1]" ] );
}

=item B<custom_query> (all)

Perform the given user query.

The query is specified with the --query parameter. The first column will be
used to perform the test for the status, if warning and critical are provided.
The warning and critical arguments can be treated as integer (default), size
or time with the --type argument. Warning and critical will be raised if they
are greater than the first column, or less than if the --reverse option is used.

All others columns will be used to generate the perfdata. The query has to
display them in the perfdata format, with unit if needed (eg. "size=35B").
If a field contains multiple values, they have to be space separated.

=cut
sub check_custom_query {
    my %args    = %{ $_[0] };
    my $me      = 'POSTGRES_CUSTOM_QUERY';
    my $sql     = $args{'query'};
    my $type    = $args{'type'} || 'integer';
    my $reverse = $args{'reverse'};
    my $bounded = undef;
    my @rs;
    my @perfdata;
    my @hosts;
    my @msg_crit;
    my @msg_warn;
    my $c_limit;
    my $w_limit;
    my $perf;
    my $value;

    # FIXME: add warn/crit threshold in perfdata

    @hosts = @{ parse_hosts %args };

    pod2usage(
        -message => 'FATAL: you must give one host with service "custom_query".',
        -exitval => 127
    ) if @hosts != 1;

    # query must be given
    pod2usage(
        -message => 'FATAL: you must set parameter "--query" with "custom_query" service.',
        -exitval => 127
    ) unless defined $args{'query'} ;

    # Handle warning and critical type
    if ($type eq 'size'){
        $w_limit = get_size( $args{'warning'} );
        $c_limit = get_size( $args{'critical'} );
    } elsif ( $type eq 'time' ){
        $w_limit = get_time( $args{'warning'} );
        $c_limit = get_time( $args{'critical'} );
    } else{
        $w_limit = $args{'warning'};
        $c_limit = $args{'critical'};
    }

    @rs = @{ query( $hosts[0], $sql ) };

DB_LOOP:    foreach my $rec (@rs) {
        $bounded = $rec->[0] if (!defined $bounded);

        $bounded = $rec->[0] if ( (!$reverse and $rec->[0] > $bounded )
            or ( $reverse and $rec->[0] < $bounded ) );

        $value = shift(@{$rec});
        if ( @{$rec} > 0){
            $perf = join(';',@{$rec})
        } else{
            $perf = undef
        }

        push @perfdata, $perf
            if ( defined $perf );

        if ( ( defined $c_limit)
            and ( !$reverse and ( $value > $c_limit )
            or ( $reverse and ( $value < $c_limit ) ) ) ){
            push @msg_crit =>
                "value: $value ".( defined $perf ? "($perf)" : "" );
            next DB_LOOP;
        }

        if ( ( defined $w_limit)
            and ( !$reverse and ( $value > $w_limit )
            or ( $reverse and ( $value < $w_limit ) ) ) ){
            push @msg_warn =>
                "value: $value ".( defined $perf ? "($perf)" : "" );
            next DB_LOOP;
        }
    }

    return critical( $me, [ @msg_crit, @msg_warn ], \@perfdata )
        if ( defined $c_limit and
            ( !$reverse and $bounded > $c_limit)
            or ( $reverse and $bounded < $c_limit) );

    return warning( $me, [ @msg_warn ], \@perfdata )
        if ( defined $w_limit and
            ( !$reverse and $bounded > $w_limit)
            or ( $reverse and $bounded < $w_limit) );

    return ok( $me, [ "Custom query ok" ], \@perfdata );
}

=item B<configuration> (8.0+)

Check the most important settings.

If warning and critical tresholds are given, they will be ignored.

Specific parameters are :
--work_mem, --maintenance_work_mem, --shared_buffers,-- wal_buffers,
--checkpoint_segments, --effective_cache_size, --no_check_autovacuum,
--no_check_fsync, --no_check_enable, --no_check_track_counts.

=cut
sub check_configuration {
    my @perfdata;
    my @hosts;
    my @msg_crit;
    my %args = %{ $_[0] };
    my $me   = 'POSTGRES_CONFIGURATION';
    # This service is based on a probe by Marc Cousin (cousinmarc@gmail.com)
    # Limit parameters. Have defaut values
    my $work_mem             = $args{'work_mem'} || 4096; # At least 4MB
    my $maintenance_work_mem = $args{'maintenance_work_mem'} || 65536; # At least 64MB
    my $shared_buffers       = $args{'shared_buffers'} || 16384; # At least 128MB
    my $wal_buffers          = $args{'wal_buffers'} || 64; # At least 512k. Or -1 for 9.1
    my $checkpoint_segments  = $args{'checkpoint_segments'} || 10;
    my $effective_cache_size = $args{'effective_cache_size'} || 131072; # At least 1GB. No way a modern server has less than 2GB of ram
    # These will be checked to verify they are still the default values (no parameter, for now)
    # autovacuum, fsync, enable*,track_counts/stats_row_level
    my $no_check_autovacuum   = $args{'no_check_autovacuum'} || 0;
    my $no_check_fsync        = $args{'no_check_fsync'} || 0;
    my $no_check_enable       = $args{'no_check_enable'} || 0;
    my $no_check_track_counts = $args{'no_check_track_counts'} || 0;

    my $sql = "SELECT name,setting FROM pg_settings
        WHERE ( ( name='work_mem' and setting::bigint < $work_mem )
            or ( name='maintenance_work_mem' and setting::bigint < $maintenance_work_mem )
            or ( name='shared_buffers' and setting::bigint < $shared_buffers )
            or ( name='wal_buffers' and ( setting::bigint < $wal_buffers or setting = '-1') )
            or ( name='checkpoint_segments' and setting::bigint < $checkpoint_segments )
            or ( name='effective_cache_size' and setting::bigint < $effective_cache_size )
            or ( name='autovacuum' and setting='off' and $no_check_autovacuum = 0)
            or ( name='fsync' and setting='off' and $no_check_fsync=0  )
            or ( name~'^enable.*' and setting='off' and $no_check_enable=0)
            or (name='stats_row_level' and setting='off' and $no_check_track_counts=0)
            or (name='track_counts' and setting='off' and $no_check_track_counts=0)
        )";

    # FIXME make one parameter --ignore to rules 'em all.

    @hosts = @{ parse_hosts %args };

    pod2usage(
        -message => 'FATAL: you must give one host with service "configuration".',
        -exitval => 127
    ) if @hosts != 1;

    is_compat $hosts[0], 'configuration', $PG_VERSION_80 or exit 1;

    my @rc = @{ query( $hosts[0], $sql ) };

DB_LOOP:    foreach my $setting (@rc) {
        push @msg_crit => ( $setting->[0] . "=" . $setting->[1] );
    }

    # All the entries in $result are an error. If the array isn't empty, we return ERROR, and the list of errors
    return critical( $me, \@msg_crit )
        if ( @msg_crit > 0 );

    return ok( $me, [ "PostgreSQL configuration ok" ] );
}

=item B<bloat> (all)

Check the table and index bloat.

Warning and critical thresholds accept either a raw number, a percentage or a
size (eg. 125M). If a percentage is given, the threshold will be applied on
each table or index.

Perfdata will give the number or tables and indexes concerned by warning and
critical threshold, and max bloat (both size and percentage) and average bloat (both
size and percentage) for tables and indexes (not filtered by thresholds).
=cut
sub check_bloat {
    my @perfdata;
    my $w_limit;
    my $c_limit;
    my @hosts;
    my @all_db;
    my %args          = %{ $_[0] };
    my $w_t_count     = 0;
    my $w_i_count     = 0;
    my $c_t_count     = 0;
    my $c_i_count     = 0;
    my $me            = 'POSTGRES_BLOAT';
    my $BLOCK_SIZE    = "(SELECT current_setting('block_size')::numeric)";
    my $USE_INHERITED = "AND s.inherited=false";

    @hosts = @{ parse_hosts %args };

    pod2usage(
        -message => 'FATAL: you must give one host with service "bloat".',
        -exitval => 127
    ) if @hosts != 1;

    set_pgversion($hosts[0]);

    $BLOCK_SIZE = "(SELECT 8192)" if $hosts[0]->{'version_num'} == $PG_VERSION_74;
    $USE_INHERITED = "" if $hosts[0]->{'version_num'} < $PG_VERSION_90;

    @all_db = @{ get_all_dbname( $hosts[0] ) };

    # This query is based on check_postgres,
    # by Greg Sabino Mullane <greg@endpoint.com>,
    # licenced under BSD
    my $sql = qq{
        SELECT DISTINCT current_database(), relkind, schemaname, relname,
          (CASE WHEN relpages <= otta THEN 0 ELSE bs*(relpages-otta)::bigint+bs*((toastpages)-(toasttuples/4))::bigint END) * 100 
            / (bs*(relpages+toastpages)::bigint) AS realbloat,
          CASE WHEN relpages <= otta THEN 0 ELSE bs*(relpages-otta)::bigint+bs*((toastpages)-(toasttuples/4))::bigint END AS wastedbytes,
          bs*(relpages+toastpages)::bigint AS totalbytes
        FROM (
          SELECT ref.relkind, tbl.oid as o1,ind.oid as o2,
            CASE WHEN ref.relkind = 'r' THEN tbl.nspname ELSE ni.nspname END AS schemaname,
            CASE WHEN ref.relkind = 'r' THEN tbl.relname ELSE ind.relname END AS relname,
            CASE WHEN ref.relkind = 'r' THEN tbl.reltuples ELSE ind.reltuples END AS reltuples,
            CASE WHEN ref.relkind = 'r' THEN tbl.relpages ELSE ind.relpages END AS relpages,
            CASE WHEN ref.relkind = 'r' THEN tbl.toastpages ELSE 0 END AS toastpages,
            CASE WHEN ref.relkind = 'r' THEN tbl.toasttuples ELSE 0 END AS toasttuples,
            tbl.bs, tbl.datahdr,
            CASE WHEN ref.relkind = 'r' THEN tbl.otta ELSE COALESCE(
                CEIL((ind.reltuples*(datahdr-12))/(bs-20::float))
                + CASE WHEN am.amname IN ('hash','btree') THEN 1 ELSE 0 END,0 -- btree and hash have a metadata reserved block
            ) END AS otta -- very rough approximation, assumes all cols
          FROM (
            SELECT
              cc.oid, nn.nspname,cc.relname, cc.reltuples, COALESCE(t.reltuples,0) as toasttuples, cc.relpages, COALESCE(t.relpages,0) as toastpages, bs AS bs, datahdr, cc.relkind,
              COALESCE(CEIL((cc.reltuples*((datahdr+ma-
                (CASE WHEN datahdr%ma=0 THEN ma ELSE datahdr%ma END))+nullhdr2+4))/(bs-20::float)),0) AS otta
            FROM
              pg_class cc
              JOIN pg_namespace nn ON cc.relnamespace = nn.oid AND nn.nspname <> 'information_schema'
              LEFT JOIN
              (
                SELECT
                  ma,bs,foo.nspname,foo.relname,
                  (datawidth+(hdr+ma-(case when hdr%ma=0 THEN ma ELSE hdr%ma END)))::numeric AS datahdr,
                  (maxfracsum*(nullhdr+ma-(case when nullhdr%ma=0 THEN ma ELSE nullhdr%ma END))) AS nullhdr2
                FROM (
                  SELECT
                    ns.nspname, tbl.relname, hdr, ma, bs,
                    SUM((1-coalesce(null_frac,0))*coalesce(avg_width, 2048)) AS datawidth,
                    MAX(coalesce(null_frac,0)) AS maxfracsum,
                    hdr+(
                      SELECT 1+count(*)/8
                      FROM pg_stats s2
                      WHERE null_frac<>0 AND s2.schemaname = ns.nspname AND s2.tablename = tbl.relname
                    ) AS nullhdr
                  FROM pg_attribute att
                  JOIN pg_class tbl ON att.attrelid = tbl.oid
                  JOIN pg_namespace ns ON ns.oid = tbl.relnamespace
                  JOIN pg_stats s ON s.schemaname=ns.nspname
                  AND s.tablename = tbl.relname
                  $USE_INHERITED
                  AND s.attname=att.attname,
                  (
                    SELECT
                      $BLOCK_SIZE as bs,
                        CASE WHEN SUBSTRING(SPLIT_PART(v, ' ', 2) FROM '#"[0-9]+#"%' for '#')::integer > 7
                          THEN 27 ELSE 23 END AS hdr,
                      CASE WHEN v ~ 'mingw32' OR v ~ '64-bit' THEN 8 ELSE 4 END AS ma
                    FROM (SELECT version() AS v) AS foo
                  ) AS constants
                  WHERE att.attnum > 0
                  GROUP BY 1,2,3,4,5
                ) AS foo
              ) AS rs
              ON cc.relname = rs.relname AND nn.nspname = rs.nspname
              LEFT JOIN pg_class t ON cc.reltoastrelid = t.oid
              WHERE cc.relkind = 'r' -- only tables, will therefore exlclude toast tables
        ) AS tbl
          CROSS JOIN (SELECT 'r' as relkind UNION SELECT 'i') as ref
          LEFT JOIN pg_index i ON indrelid = tbl.oid
          LEFT JOIN pg_class ind ON ind.oid = i.indexrelid AND ref.relkind = ind.relkind AND ind.relkind = 'i'
          LEFT JOIN pg_namespace ni ON ind.relnamespace = ni.oid
          LEFT JOIN pg_am am ON ind.relam = am.oid
        WHERE NOT (ref.relkind = 'i' AND ind.oid IS NULL) -- handle tables without index
        ) AS sql
        WHERE ROUND(CASE WHEN otta=0 OR relpages=0 OR relpages=otta THEN 0.0 ELSE relpages/otta::numeric END,1) > 1.0
        order by 4 desc
    };

# Iterate over all db
ALLDB_LOOP: foreach my $db (@all_db) {
        my @rc = @{ query( $hosts[0], $sql, $db ) };
        # Var to handle max,avg and count for size and percentage, per relkind
        my $nb_ind = 0;
        my $max_ind_size = 0;
        my $sum_ind_size = 0;
        my $avg_ind_size = 0;
        my $max_ind_percent = 0;
        my $sum_ind_percent = 0;
        my $avg_ind_percent = 0;
        my $nb_tab = 0;
        my $max_tab_size = 0;
        my $sum_tab_size = 0;
        my $avg_tab_size = 0;
        my $max_tab_percent = 0;
        my $sum_tab_percent = 0;
        my $avg_tab_percent = 0;
        BLOAT_LOOP:    foreach my $bloat (@rc) {
            # We need to calculate threshold on each object, has the value can be given in percetage
            $w_limit = get_size( $args{'warning'}, $bloat->[6] );
            $c_limit = get_size( $args{'critical'}, $bloat->[6] );

            $w_t_count++ if ($bloat->[1] eq 'r' and $bloat->[5] > $w_limit );
            $w_i_count++ if ($bloat->[1] eq 'i' and $bloat->[5] > $w_limit );

            $c_t_count++ if ($bloat->[1] eq 'r' and $bloat->[5] > $c_limit );
            $c_i_count++ if ($bloat->[1] eq 'i' and $bloat->[5] > $c_limit );

            if ( $bloat->[1] eq "i" ){
                $nb_ind++;
                $max_ind_size = $bloat->[5] if ( $bloat->[5] > $max_ind_size);
                $sum_ind_size += $bloat->[5];
                $max_ind_percent = $bloat->[4] if ( $bloat->[4] > $max_ind_percent);
                $sum_ind_percent += $bloat->[4];
            } else {
                $nb_tab++;
                $max_tab_size = $bloat->[5] if ( $bloat->[5] > $max_tab_size);
                $sum_tab_size += $bloat->[5];
                $max_tab_percent = $bloat->[4] if ( $bloat->[4] > $max_tab_percent);
                $sum_tab_percent += $bloat->[4];
            }
        }
        # Display "NaN" values to make sure grapher will break the line.
        if ( $nb_ind == 0 ){
                $max_ind_size = "NaN";
                $sum_ind_size = "NaN";
                $avg_ind_size = "NaN";
                $max_ind_percent = "NaN";
                $sum_ind_percent = "NaN";
                $avg_ind_percent = "NaN";
        } else{
                $avg_ind_size = ( $sum_ind_size / $nb_ind );
                $avg_ind_percent = ( $sum_ind_percent / $nb_ind )
        }
        if ( $nb_tab == 0 ){
                $max_tab_size = "NaN";
                $sum_tab_size = "NaN";
                $avg_tab_size = "NaN";
                $max_tab_percent = "NaN";
                $sum_tab_percent = "NaN";
                $avg_tab_percent = "NaN";
        } else{
                $avg_tab_size = ( $sum_tab_size / $nb_tab );
                $avg_tab_percent = ( $sum_tab_percent / $nb_tab )
        }
        # Construc perfdata
        my $perf = "${db}_i_max_s=${max_ind_size}B ${db}_i_avg_s=${avg_ind_size}B"
            . " ${db}_i_max_p=${max_ind_percent}% ${db}_i_avg_p=${avg_ind_percent}%"
            . " ${db}_t_max_s=${max_tab_size}B ${db}_t_avg_s=${avg_tab_size}B"
            . " ${db}_t_max_p=${max_tab_percent}% ${db}_t_avg_p=${avg_tab_percent}%";
        push @perfdata => $perf;
    }

    return critical( $me, [ $c_t_count . " table" . ( $c_t_count > 1 ? "s" : "" ) . " " . $c_i_count . " index" . ( $c_i_count > 1 ? "es" : "" ) ], [ "t_count=$c_t_count i_count=$c_i_count", @perfdata ] ) if ( ( $c_t_count + $c_i_count ) > 0 );
    return warning( $me, [ $w_t_count . " table" . ( $w_t_count > 1 ? "s" : "" ) . " " . $w_i_count . " index" . ( $w_i_count > 1 ? "es" : "" ) ], [ "t_count=$w_t_count i_count=$w_i_count", @perfdata ] ) if ( ( $w_t_count + $w_i_count ) > 0 );
    return ok( $me, [ "Database bloat ok" ], \@perfdata );
}

# End of SERVICE section in pod doc
=pod

=back

=cut

Getopt::Long::Configure('bundling');
GetOptions(
    \%args,
        'service|s=s',
        'host|h=s',
        'username|U=s',
        'port|p=s',
        'dbname|d=s',
        'dbservice|S=s',
        'warning|w=s',
        'critical|c=s',
        'exclude=s',
        'psql|P=s',
        'path=s',
        'status-file=s',
        'query=s',
        'type=s',
        'reverse!',
        'work_mem=i',
        'maintenance_work_mem=i',
        'shared_buffers=i',
        'wal_buffers=i',
        'checkpoint_segments=i',
        'effective_cache_size=i',
        'no_check_autovacuum!',
        'no_check_fsync!',
        'no_check_enable!',
        'no_check_track_counts!',
        'list!',
        'version|V!',
        'help|?!',
        'debug!'
) or pod2usage( -exitval => 127 );

list_services() if $args{'list'};
version()       if $args{'version'};

pod2usage( -verbose => 2 ) if $args{'help'};


# One service must be given
pod2usage(
    -message => "FATAL: you must specify one service.\n"
        . "    See -s or --service command line option.",
    -exitval => 127
) unless defined $args{'service'};


# Check that the given service exists.
pod2usage(
    -message => "FATAL: service $args{'service'} does not exist.\n"
        . "    Use --list to show the available services.",
    -exitval => 127
) unless exists $services{ $args{'service'} };


# Critical and Warning must be given
pod2usage(
    -message => 'FATAL: you must specify critical and warning thresholds.',
    -exitval => 127
) unless ( ($args{'service'} eq 'minor_version')
    or ($args{'service'} eq 'connection')
    or ($args{'service'} eq 'custom_query')
    or ($args{'service'} eq 'configuration')
    )
    or (defined $args{'warning'} and $args{'critical'});


# Both critical and warning must be given is optionnal
pod2usage(
    -message => 'FATAL: you must provide both warning and critical thresholds.',
    -exitval => 127
) if ( ( defined $args{'critical'} and !defined $args{'warning'} ) or ( !defined $args{'critical'} and defined $args{'warning'} ) );


# query, type and reverse are only allowed with "custom_query" service
pod2usage(
    -message => 'FATAL: query, type and reverse are only allowed with "custom_query" service.',
    -exitval => 127
) if ( ( defined $args{'query'} or defined $args{'type'} or $args{'reverse'} == 1 ) and ( $args{'service'} ne 'custom_query' ) );


# Check "configuration" specific arg
pod2usage(
    -message => 'FATAL: work_mem, maintenance_work_mem, shared_buffers, wal_buffers, checkpoint_segments, effective_cache_size, no_check_autovacuum, no_check_fsync, no_check_enable, no_check_track_counts are only allowed with "configuration" service.',
    -exitval => 127
) if ( (defined $args{'work_mem'} or defined $args{'maintenance_work_mem'} or defined $args{'shared_buffers'}
    or defined $args{'wal_buffers'} or defined $args{'checkpoint_segments'} or defined $args{'effective_cache_size'}
    or $args{'no_check_autovacuum'} == 1 or $args{'no_check_fsync'} == 1 or $args{'no_check_enable'} ==1
    or $args{'no_check_track_counts'} == 1) and ( $args{'service'} ne 'configuration' ) );


exit $services{ $args{'service'} }{'sub'}->( \%args );

__END__

=head1 EXAMPLES

=over

=item C<check_pgactivity -h localhost -p 5492 -s last_vacuum -w 30m -c 1h30m>

Execute service "last_vacuum" on host "host=localhost port=5432".

=item C<check_pgactivity --debug --dbservice pg92,pg92s --service streaming_delta -w 60 -c 90>

Execute service "streaming_delta" between hosts "service=pg92" and "service=pg92s".

=item C<check_pgactivity --debug --dbservice pg92 -h slave -U supervisor --service streaming_delta -w 60 -c 90>

Execute service "streaming_delta" between hosts "service=pg92" and "host=slave user=supervisor".

=back

=head1 LICENSING

This program is open source, licensed under the simplified BSD license. For
license terms, see the LICENSE provided with the sources.

=head1 AUTHORS

Author: Jehan-Guillaume de Rorthais
Copyright: (C) 2012 Jehan-Guillaume de Rorthais - All rights reserved.

Dalibo's team. http://www.dalibo.org
  
=cut
